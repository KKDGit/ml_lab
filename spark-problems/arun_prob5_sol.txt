/*
Sqoop is one of the important topics for the exam. 
Based on generally reported exam pattern from anonymous internet bloggers, you can expect 2 out of 10 questions on this topic related to Data Ingest and Data Export using Sqoop. 
Hence, 20% of the exam score can be obtained just by practicing simple Sqoop concepts. 
Sqoop can be mastered easily (i.e in a few hours) at the skill level that CCA 175 exam is expecting you to demonstrate. 
I created this problem focusing on Sqoop alone, if you are able to perform this exercise on your own or at worst using just the sqoop user guide then there is a very very high chance that you can score the 20% easily.

Pre-Work: Please perform these steps before solving the problem
1. Login to MySQL using below commands on a fresh terminal window
    mysql -u retail_dba -p
    Password = cloudera
2. Create a replica product table and name it products_replica
    create table products_replica as select * from products
3. Add primary key to the newly created table
    alter table products_replica add primary key (product_id);
4. Add two more columns
    alter table products_replica add column (product_grade int, product_sentiment varchar(100))
5. Run below two update statements to modify the data
    update products_replica set product_grade = 1  where product_price > 500;
    update products_replica set product_sentiment  = 'WEAK'  where product_price between 300 and  500;
  ------------------------
use retail_export;
show tables like'%kkd%';
show tables like '%kranthi%';

create table kkd_products_replica as select * from retail_db.products;
describe kkd_products_replica;

alter table kkd_products_replica add primary key (product_id);
alter table kkd_products_replica add column (product_grade int, product_sentiment varchar(100))
describe kkd_products_replica;

update kkd_products_replica set product_grade=1 where product_price > 500;
update kkd_products_replica set product_sentiment='WEAK' where product_price between 200 and 500;
select * from kkd_products_replica limit 10;
-----------------------------------------------------------------
Problem 5: 
Above steps are important so please complete them successfully before attempting to solve the problem
++++++++++++++++++++++++++++
How to read a file with line delimited than other formats

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--target-dir /user/kranthidr/arun/problem5/prod_text_pipestar --as-textfile \
--fields-terminated-by '|' --lines-terminated-by '#' \
--where "product_id between 1 and 1000" \
--num-mappers 6 \
--null-string 'NULL' --null-non-string 'NULL' \
--columns "product_id,product_category_id,product_price"

val ip3 = spark.read.csv("/user/kranthidr/arun/problem5/prod_text_pipestar").toDF("part")
val ip4 = ip3.select(explode(split('part,"#")).as("value"))
val ip5 = ip4.select(split('value,"\\|").as("cols")).select('cols(0),'cols(1),'cols(2))

val cols = ip5.columns.map(c => count(when(col(c).isNull, c)).alias("cn_"+c))
ip5.select(cols:_*).show(false)
ip5.na.drop().count()
----------------------------------
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.io.Text
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat

val conf = new Configuration
conf.set("textinputformat.record.delimiter", "#")

val log_df = sc.newAPIHadoopFile(
"/user/kranthidr/arun/problem5/prod_text_pipestar", 
classOf[TextInputFormat], 
classOf[LongWritable], 
classOf[Text], 
conf
).map(_._2.toString)

log_df.map(x => x.split("\\|")).map(x => 
(x(0).toInt, x(1).toInt, x(2).toFloat)).
toDF("product_id,product_category_id,product_price".split(","):_*).show()

------------------------------------------------------------


1.
Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '|' and lines are separated by '\n'. Null values are represented as -1 for numbers and "NOT-AVAILABLE" for strings. Only records with product id greater than or equal to 1 and less than or equal to 1000 should be imported and use 3 mappers for importing. The destination file should be stored as a text file to directory  /user/cloudera/problem5/products-text. 

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--target-dir /user/kranthidr/arun/problem5/products-text --as-textfile \
--fields-terminated-by '|' --lines-terminated-by '\n' \
--where "product_id between 1 and 1000" \
--num-mappers 3 \
--null-string 'NOT-AVAILABLE' --null-non-string -1
2.
Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '*' and lines are separated by '\n'. Null values are represented as -1000 for numbers and "NA" for strings. Only records with product id less than or equal to 1111 should be imported and use 2 mappers for importing. The destination file should be stored as a text file to directory  /user/cloudera/problem5/products-text-part1. 

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--target-dir /user/kranthidr/arun/problem5/products-text-part1 --as-textfile \
--fields-terminated-by '*' --lines-terminated-by '\n' \
--where "product_id <= 1111" \
--num-mappers 2 \
--null-string 'NA' --null-non-string -1000

3.
Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '*' and lines are separated by '\n'. Null values are represented as -1000 for numbers and "NA" for strings. Only records with product id greater than 1111 should be imported and use 5 mappers for importing. The destination file should be stored as a text file to directory  /user/cloudera/problem5/products-text-part2.

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--target-dir /user/kranthidr/arun/problem5/products-text-part2 --as-textfile \
--fields-terminated-by '*' --lines-terminated-by '\n' \
--where "product_id > 1111" \
--num-mappers 5 \
--null-string 'NA' --null-non-string -1000

4.
Using sqoop merge data available in /user/cloudera/problem5/products-text-part1 and /user/cloudera/problem5/products-text-part2 to produce a new set of files in /user/cloudera/problem5/products-text-both-parts

sqoop codegen \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--outdir /home/kranthidr/arun

sqoop merge \
--new-data /user/kranthidr/arun/problem5/products-text-part2 \
--onto /user/kranthidr/arun/problem5/products-text-part1 \
--merge-key product_id \
--target-dir /user/kranthidr/arun/problem5/products-text-both-parts

5.
Using sqoop do the following. Read the entire steps before you create the sqoop job.
create a sqoop job Import Products_replica table as text file to directory /user/cloudera/problem5/products-incremental. 
Import all the records.
insert three more records to Products_replica from mysql
run the sqoop job again so that only newly added records can be pulled from mysql
insert 2 more records to Products_replica from mysql
run the sqoop job again so that only newly added records can be pulled from mysql

Validate to make sure the records have not be duplicated in HDFS

sqoop job --create import_products_incr -- import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--target-dir /user/kranthidr/arun/problem5/products-incremental \
--check-column product_id \
--incremental append \
--last-value 0

sqoop job --exec import_products_incr

mysql> insert into kkd_products_replica values (1346,2,'something 1','something 2',300.00,'not avaialble',3,'STRONG');
mysql> insert into kkd_products_replica values (1347,5,'something 787','something 2',356.00,'not avaialble',3,'STRONG');

sqoop job --exec import_products_incr

mysql> insert into kkd_products_replica values (1376,4,'something 1376','something 2',1.00,'not avaialble',3,'WEAK');
mysql> insert into kkd_products_replica values (1365,4,'something 1376','something 2',10.00,'not avaialble',null,'NOT APPLICABLE');

sqoop job --exec import_products_incr

mysql> insert into kkd_products_replica values (1350,4,'something 1376','something 2',1.00,'not avaialble',3,'WEAK');
mysql> insert into kkd_products_replica values (1351,4,'something 1376','something 2',10.00,'not avaialble',null,'NOT APPLICABLE');
above will not work

Go to your pwd
cd .sqoop
open file metastore.db.script using vi or your fav editor.
search for incremental.last.value

6.
Using sqoop do the following. Read the entire steps before you create the sqoop job.

create a hive table in database named kkd_problem5 using below command 
hive> create database kkd_problem5 location '/user/kranthidr/arun/kkd_problem5';
hive> use kkd_problem5;
hive> create table products_hive  (product_id int, product_category_id int, product_name string, product_description string, product_price float, product_image string, product_grade int,  product_sentiment string);

create a sqoop job import retail_export.kkd_products_replica table as hive table to database named kkd_problem5. Name the table as products_hive

sqoop job --delete hive_import_products_incr

sqoop job --create hive_import_products_incr -- import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_replica \
--hive-import --hive-database kkd_problem5 --hive-table products_hive \
--check-column product_id \
--incremental append \
--last-value 0 \
--null-string '\\N' \
--null-non-string '\\N'

sqoop job --exec hive_import_products_incr

insert three more records to kkd_products_replica from mysql

mysql> insert into kkd_products_replica values (1377,2,'something 1377','something 2',300.00,'not avaialble',3,'STRONG'), (1378,5,'something 1378','something 2',356.00,'not avaialble',3,'STRONG'), (1379,2,'something 1379','something 2',300.00,'not avaialble',3,'STRONG'), (1380,5,'something 1380','something 2',356.00,'not avaialble',3,'STRONG');

run the sqoop job again so that only newly added records can be pulled from mysql
sqoop job --exec hive_import_products_incr


insert 2 more records to Products_replica from mysql

insert into kkd_products_replica values (1381,2,'something 1381','something 2',300.00,'not avaialble',3,'STRONG'), (1382,5,'something 1382','something 2',356.00,'not avaialble',3,'STRONG'), (1384,2,'something 1384','something 2',300.00,'not avaialble',3,'STRONG');

run the sqoop job again so that only newly added records can be pulled from mysql
sqoop job --exec hive_import_products_incr

Validate to make sure the records have not been duplicated in Hive table

7.
Using sqoop do the following.
insert 2 more records into products_hive table using hive. 
hive> insert into products_hive values (1385,2,'something 1385','something 2',300.00,'not avaialble',3,'STRONG'), (1386,5,'something 1386','something 2',356.00,'not avaialble',3,'STRONG');

create table in mysql using below command  

mysql> create table kkd_products_external  (product_id int(11) primary Key, product_grade int(11), product_category_id int(11), product_name varchar(100), product_description varchar(100), product_price float, product_image varchar(500), product_sentiment varchar(100));

export data from products_hive (hive) table to (mysql) kkd_products_external table.

sqoop job --delete hive_export_products_incr

sqoop job --create hive_export_products_incr -- export \
--connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity \
--table kkd_products_external \
--hcatalog-database kkd_problem5  --hcatalog-table products_hive \
--update-key product_id \
--update-mode allowinsert

// --update-mode has two option allowinsert- it will not insert if record is not there... updateonly -- only updates 

sqoop job --exec hive_export_products_incr

insert 2 more records to Products_hive table from hive

hive> insert into products_hive values (1387,2,'something 1387','something 2',300.00,'not avaialble',3,'STRONG'), (1388,5,'something 1388','something 2',356.00,'not avaialble',3,'STRONG');
hive> insert into products_hive values (1389,2,'something 1389','something 2',300.00,'not avaialble',3,'STRONG'), (1390,5,'something 1390','something 2',356.00,'not avaialble',3,'STRONG');

hive>
insert into products_hive values (1391,2,'something 1391','something 2',300.00,'not avaialble',3,'STRONG'), (1392,5,'something 1392','something 2',356.00,'not avaialble',3,'STRONG');

export data from products_hive table to products_external table.

sqoop job --exec hive_export_products_incr

Validate to make sure the records have not be duplicated in mysql table




