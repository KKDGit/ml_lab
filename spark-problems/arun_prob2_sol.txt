/*
1.
Using sqoop copy data available in mysql products table to folder /user/kranthidr/arun/products on hdfs as text file. 
columns should be delimited by pipe '|'
move all the files from /user/kranthidr/arun/products folder to /user/kranthidr/arun/problem2/products folder

2.
Change permissions of all the files under /user/kranthidr/arun/problem2/products such that owner has read,write and execute permissions, 
group has read and write permissions whereas others have just read and execute permissions //read = 4, write = 2, execute = 1..765
*/

hadoop fs -mkdir -p /user/kranthidr/arun/problem2

sqoop import \
--connect 'jdbc:mysql://ms.itversity.com:3306/retail_db' \
--username retail_user --password itversity \
--table products \
--target-dir '/user/kranthidr/arun/products' \
--fields-terminated-by '|' \
--as-textfile

hadoop fs -cp /user/kranthidr/arun/products /user/kranthidr/arun/problem2/

hadoop fs -chmod -R 765 /user/kranthidr/arun/problem2/products

=======================================================================
/*
3.
read data in /user/cloudera/problem2/products and do the following operations using 
a) dataframes api 
b) spark sql 
c) RDDs aggregateByKey method. 
Your solution should have three sets of steps. 

4.
Sort the resultant dataset by category id

5.
filter such that your RDD\DF has products whose price is lesser than 100 USD

6.
on the filtered data set find out the higest value in the product_price column under each category
on the filtered data set also find out total products under each category
on the filtered data set also find out the average price of the product under each category
on the filtered data set also find out the minimum price of the product under each category

7.
store the result in avro file using snappy compression under these folders respectively

/user/cloudera/problem2/products/result-df
/user/kranthidr/arun/problem2/products/result-sql
/user/kranthidr/arun/problem2/products/result-rdd

*/
//DF

case class Product(product_id:Int, product_category_id:Int, product_name:String, product_description:String ,product_price:Float, product_image:String)

import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.catalyst.ScalaReflection
import com.databricks.spark.avro._
import org.apache.spark.sql.functions._

val my_schema = ScalaReflection.schemaFor[Product].dataType.asInstanceOf[StructType]

val products = spark.read.schema(my_schema).option("delimiter", "|").csv("/user/kranthidr/arun/problem2/products").filter("product_price < 100").orderBy('product_category_id.asc)

val max_priced_categ = products.groupBy('product_category_id).max("product_price")

val tot_prods_categ = products.groupBy('product_category_id).agg(count("product_id"))

val avg_price_categ = products.groupBy('product_category_id).avg("product_price")

val min_priced_categ = products.groupBy('product_category_id).min("product_price")

val cat_analysis = products.groupBy('product_category_id).agg(countDistinct("product_id").as("count"), max("product_price").as("max_price"),  avg("product_price").as("avg_price"), min("product_price").as("min_price"))

cat_analysis.write.mode("overwrite").option("compression", "snappy").avro("/user/kranthidr/arun/problem2/result-df")

//SQL

products.registerTempTable("products")

var sqlResult = spark.sql("select product_category_id, max(product_price) as maximum_price, count(distinct(product_id)) as total_products, cast(avg(product_price) as decimal(10,2)) as average_price, min(product_price) as minimum_price from products where product_price <100 group by product_category_id order by product_category_id desc")

sqlResult.show()

//RDD

var rddResult = products.rdd.map(x => (x(1).toString.toInt, x(4).toString.toDouble)).
aggregateByKey((0.0, 0.0, 0, 9999999999999.0))(
(x, y) => (math.max(x._1, y), x._2 + y, x._3 + 1, math.min(x._4, y)),
(x, y) => (math.max(x._1, y._1), x._2 + y._2, x._3 + y._3, math.min(x._4, y._4))
).
map(x => (x._1, x._2._1, (x._2._2/x._2._3), x._2._3, x._2._4)).
sortBy(_._1, false)

rddResult.collect().foreach(println)
