/*
Problem 3: Perform in the same sequence

1.
Import all tables from mysql database into hdfs as avro data files. use compression and the compression codec should be snappy. data warehouse directory should be retail_stage.db

sqoop import-all-tables \
--connect 'jdbc:mysql://ms.itversity.com:3306/retail_db' --username retail_user --password itversity \
--compress --compression-codec snappy --as-avrodatafile \
--warehouse-dir /user/kranthidr/arun/problem3/retail_stage.db \
--num-mappers 1

2.
Create a metastore table that should point to the orders data imported by sqoop job above. 

Name the table orders_sqoop.

avro-tools getschema 'hdfs://nn01.itversity.com:8020/user/kranthidr/arun/problem3/retail_stage.db/orders/part-m-00000.avro' > orders.avsc

hadoop fs -put orders.avsc /user/kranthidr/arun/problem3/retail_stage.db/orders.avsc

create database kkd_problem3 location '/user/kranthidr/arun/kkd_problem3';

use kkd_problem3;

CREATE EXTERNAL TABLE kkd_orders_sqoop
STORED AS AVRO
LOCATION '/user/kranthidr/arun/problem3/retail_stage.db/orders'
TBLPROPERTIES ('avro.schema.url'='/user/kranthidr/arun/problem3/retail_stage.db/orders.avsc');


CREATE EXTERNAL TABLE kkd_orders_sqoop
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION '/user/kranthidr/arun/problem3/retail_stage.db/orders'
TBLPROPERTIES ('avro.schema.url'='/user/kranthidr/arun/problem3/retail_stage.db/orders.avsc');

3.
Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed. 
select data from orders_sqoop. 

spark.sql("use kkd_problem3")
val orders = spark.sql("select * from kkd_orders_sqoop")
//val ords = orders.withColumn("date", to_date(to_timestamp('order_date/1000)))
val ords = orders.withColumn("date", to_date(from_unixtime('order_date/1000)))
//both are same

val big_day = ords.groupBy('date).agg(countDistinct('order_id).as("count")).orderBy('count.desc).select("date").limit(1).collect()(0)(0).toString

ords.where('date === big_day).count() //347 
ords.where('date === big_day).show(500)

HIVE
1.
select order_date, count(distinct(order_id)) count from kkd_orders_sqoop group by order_date order by count desc limit 1;

2.
select bd.order_date from (select order_date, count(distinct(order_id)) count from kkd_orders_sqoop group by order_date order by count desc limit 1) bd;

select * from kkd_orders_sqoop where order_date IN (select bd.order_date from (select order_date, count(distinct(order_id)) count from kkd_orders_sqoop group by order_date order by count desc limit 1) bd);

4.
query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed. 
select data from order_sqoop. 

5.
Now create a table named retail.orders_avro in hive stored as avro, the table should have same table definition as order_sqoop. Additionally, this new table should be partitioned by the order month i.e -> year-order_month.(example: 2014-01)
6.
Load data into orders_avro table from orders_sqoop table.

hive>
create table orders_avro (order_id int, order_date bigint, order_customer_id int, order_status string) partitioned by (order_month string) stored as avro;

select order_id, order_date, order_customer_id, order_status, from_unixtime(cast(order_date/1000 as int)) order_month  from kkd_orders_sqoop;

set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table orders_avro partition(order_month) select order_id, order_date, order_customer_id, order_status, substr(from_unixtime(cast(order_date/1000 as int)),1,7) as order_month  from kkd_orders_sqoop;

------------------
//spark.sql("use kkd_problem3")
//val orders = spark.sql("select * from kkd_orders_sqoop")
//val ords = orders.withColumn("order_month", date_format(from_unixtime('order_date/1000),"yyyy-MM"))

//ords.write.mode("overwrite").format("avro").partitionBy("order_month").saveAsTable("orders_avro1")
//Not able to read from Hive but can be read from Spark
//DO NOT USE THIS
-------------------------------------------------------
spark.sql("use kkd_problem3")

spark.sql("DROP TABLE IF EXISTS orders_avro1")

spark.sql("CREATE TABLE orders_avro1 (order_id int, order_date bigint, order_customer_id int, order_status string) partitioned by (order_month string) stored as avro")

val orders = spark.sql("select * from kkd_orders_sqoop")
val ords = orders.drop("order_style", "order_zone").withColumn("order_month", date_format(from_unixtime('order_date/1000),"yyyy-MM"))

spark.conf.set("hive.exec.dynamic.partition", "true")
spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
ords.write.insertInto("kkd_problem3.orders_avro1")
-------------------------------------------------------------------------

7.
Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed. 
select data from orders_avro

8.
evolve the avro schema related to orders_sqoop table by adding more fields named (order_style String, order_zone Integer)
,{
    "name" : "order_style",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "order_style",
    "sqlType" : "12"
  }, {
    "name" : "order_zone",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_zone",
    "sqlType" : "4"
  }
 add this to .avsc file in fields JSON category
 
9.
insert two more records into orders_sqoop table.

insert into table kkd_orders_sqoop values (8888888,1374735600000,11567,"xyz",9,"CLOSED"),(8888889,1374735600000,11567,"xyz",9,"CLOSED");

10.
Write query in hive that shows all orders belonging to a certain day. 
This day is when the most orders were placed. select data from orders_sqoop

select * from kkd_orders_sqoop where order_date IN (select bd.order_date from (select order_date, count(distinct(order_id)) count from kkd_orders_sqoop group by order_date order by count desc limit 1) bd);

11.
query table in impala that shows all orders belonging to a certain day. 
This day is when the most orders were placed. select data from orders_sqoop

Lanch Impala shell by using command impala-shell

1. Run 'Invalidate metadata'
2. Run below query

select * from kkd_orders_sqoop as X where X.order_date in (select a.order_date from (select Y.order_date, count(1) as total_orders from orders_sqoop as Y group by Y.order_date order by total_orders desc, Y.order_date desc limit 1) a);


*/