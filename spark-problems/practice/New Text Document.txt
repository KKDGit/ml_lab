Background
#Use itversity labs tables (orders,orderitems,products)
#Use Scala/Spark RDD/Spark SQL/DF-DataFrame/Hive SQL
#Some functions you need to be familiar with to solve these
reduceByKey
sortByKey
groupByKey
aggregateByKey
#Learn/Remember these calls
spark-shell --master yarn --packages com.databricks:spark-avro_2.10:2.0.1 --conf spark.ui.port=xxxxxx
sqlContext.setConf(“spark.sql.avro.compression.codec”,“snappy”)
sqlContext.setConf(“spark.sql.parquet.compression.codec”,“gzip”) (or snappy)
sqlContext.setConf(“spark.sql.xxxxx.compression.codec”,“uncompressed”) --> very important to know how to reset. if not you can open new spark shell for every problem to be on the safer side so that you dont mix up compression/no compression

someDF.registerTempTable(“temptablename”)
sqlContext.sql(“select * from temptablename”).show
var someDF=sqlContext.sql(“select * from temptablename”)
someDF.write.avro / someDF.write.parquet
someDF.saveAsTable(“hivetablename”)

var hc = new org.apache.spark.sql.hive.HiveContext(sc);
var someDF=hc.sql(“select xxxxxx dbname.tablename”)

To handle output as sequence… RDD(K,V) . K and V should be string

#You can solve some of these problems either using RDD/DF functions or using spark sql
My way of doing is… if only one table involved or involves 2 tables with simple join – Do it in RDD/DF/Function/join/leftouterjoin
If 2 or more tables involved with complex joins. convert DFs to temp tables and do the joins with sql and save the output
++++++++++++++++++++++++++++++++
spark-shell --master yarn --conf spark.ui.port=12345 –num-executors 10 –executor-cores 2 –executor-memory 3G –packages com.databricks:spark-avro_2.10:2.0.1
spark-shell --packages com.databricks:spark-csv_2.11:1.2.0

val ordersRDD = sc.textFile("sqoop_import/orders")
case class Orders(order_id: Integer, order_date: String, order_customer_id: Integer, order_status: String)
val ordersDF = ordersRDD.map(rec => rec.split(",")).map(rec => Orders(rec(0).toInt, rec(1), rec(2).toInt, rec(3))).toDF()
ordersDF.registerTempTable("orders_df")	

val orderItemsRDD = sc.textFile("sqoop_import/order_items")
case class OrderItems(order_item_id: Integer, order_item_order_id: Integer, order_item_product_id: Integer, order_item_quantity: Integer, order_item_subtotal: Float, order_item_product_price: Float)
val orderItemsDF = orderItemsRDD.map(rec => rec.split(",")).map(rec => OrderItems(rec(0).toInt, rec(1).toInt, rec(2).toInt, rec(3).toInt, rec(4).toFloat, rec(5).toFloat)).toDF()
orderItemsDF.registerTempTable("order_items_df")

val customersRDD = sc.textFile("sqoop_import/customers")
case class Customers(customer_id:Integer, customer_name:String)
val customersDF = customersRDD.map(rec => rec.split(",")).map(rec => Customers(rec(0).toInt, rec(1).concat(", ").concat(rec(2)))).toDF()
customersDF.registerTempTable("customers_df")

// set number of shuffle partitions to 10
sqlContext.sql("set spark.sql.shuffle.partitions=10");

val fullDataDF = sqlContext.sql("select customers_df.customer_name as name, round(sum(order_items_df.order_item_subtotal),2) as amount from orders_df, order_items_df, customers_df where orders_df.order_id = order_items_df.order_item_id and orders_df.order_customer_id = customers_df.customer_id group by customers_df.customer_name")

// ===================================================================================================================================================================================================================================================================================================================================================================
// WRITE TO TEXT FILE
// save as text file output with pipe as delimiter

fullDataDF.rdd.map(rec => rec.mkString(" | ")).take(10).foreach(println)
fullDataDF.rdd.map(rec => rec.mkString(" | ")).coalesce(1).saveAsTextFile("scala_customer_orders_text_pipe_data")

// save as text file output with tab as delimiter
fullDataDF.rdd.map(rec => rec.mkString(" \t ")).take(10).foreach(println)
fullDataDF.rdd.map(rec => rec.mkString(" \t ")).coalesce(1).saveAsTextFile("scala_customer_orders_text_tab_data")

// save as text file output with comma as delimiter (CSV)
fullDataDF.rdd.map(rec => rec.mkString(" , ")).coalesce(1).saveAsTextFile("scala_customer_orders_text_comma_data")
fullDataDF.rdd.map(rec => rec.mkString(" , ")).coalesce(1).saveAsTextFile("scala_customer_orders_text_comma_data", classOf[org.apache.hadoop.io.compress.SnappyCodec])

// ===================================================================================================================================================================================================================================================================================================================================================================
// WRITE TO PARQUET FILE
// save as parquet file with default compression as gz
fullDataDF.write.parquet("scala_customer_orders_parquet")

// save as parquet file with compression as uncompressed
sqlContext.setConf("spark.sql.parquet.compression.codec", "uncompressed")
fullDataDF.write.parquet("scala_customer_orders_parquet_uncomp")

// save as parquet file with compression as snappy
sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
fullDataDF.write.parquet("scala_customer_orders_parquet_snappy")

// READ FROM PARQUET FILE
// file can be read irrespective of compression
val parquetDF = sqlContext.read.parquet("scala_customer_orders_parquet_snappy")
val parquetDF = sqlContext.read.format("parquet").load("scala_customer_orders_parquet_snappy")

// ===================================================================================================================================================================================================================================================================================================================================================================
// WRITE TO JSON FILE
// save as json output file
fullDataDF.write.format("json").save("scala_customer_orders_direct_json")
fullDataDF.toJSON.coalesce(1).saveAsTextFile("scala_customer_orders_json")

// save as json output file with Snappy Compression
fullDataDF.toJSON.coalesce(1).saveAsTextFile("scala_customer_orders_json_snappy", classOf[org.apache.hadoop.io.compress.SnappyCodec])

// READ FROM JSON FILE
// file can be read irrespective of compression
val jsonDF = sqlContext.read.json("scala_customer_orders_json")
val jsonDF = sqlContext.read.format("json").load("scala_customer_orders_json")

// ===================================================================================================================================================================================================================================================================================================================================================================
// WRITE TO ORC FILE
// save as orc file
fullDataDF.write.format("orc").save("scala_customer_orders_orc")

sqlContext.setConf("spark.sql.orc.compression.codec", "snappy")
fullDataDF.write.format("orc").save("scala_customer_orders_orc_snappy")

// READ FROM ORC FILE
// file can be read irrespective of compression
val orcDF = sqlContext.read.orc("scala_customer_orders_orc")
val orcDF = sqlContext.read.format("orc").load("scala_customer_orders_orc")
// ===================================================================================================================================================================================================================================================================================================================================================================
// WRITE TO AVRO FILE
// save as avro file
import com.databricks.spark.avro._;
fullDataDF.write.avro("scala_customer_orders_avro")

sqlContext.setConf("spark.sql.snappy.compression.codec", "snappy")
fullDataDF.write.avro("scala_customer_orders_avro_snappy")

// READ FROM AVRO FILE
// file can be read irrespective of compression
val avroDF = sqlContext.read.avro("scala_customer_orders_avro_snappy")
val avroDF = sqlContext.read.format("com.databricks.spark.avro").load("scala_customer_orders_avro")
// ===================================================================================================================================================================================================================================================================================================================================================================
File Format	Action	Procedure and points to remember	Example
TEXT FILE	READ	sc.textFile(<path to file>);	 
	WRITE	rdd.saveAsTextFile(<path to file>,classOf[compressionCodecClass]);	 
		//use any codec here org.apache.hadoop.io.compress.(BZip2Codec or GZipCodec or SnappyCodec)
 
SEQUENCE FILE	READ	sc.sequenceFile(<path location>,classOf[<class name>],classOf[<compressionCodecClass >]);	 
		//read the head of sequence file to understand what two class names need to be used here	 
	WRITE	rdd.saveAsSequenceFile(<path location>, Some(classOf[compressionCodecClass]))

Available Only for Paired RDD, other wise saveAsObjectFile	 
		//use any codec here (BZip2Codec,GZipCodec,SnappyCodec)
 
		//here rdd is MapPartitionRDD and not the regular pair RDD.	 
PARQUET FILE	READ	//use data frame to load the file.	 
		sqlContext.read.parquet(<path to location>); //this results in a data frame object.	 
	WRITE	sqlContext.setConf("spark.sql.parquet.compression.codec","gzip") //use gzip, snappy, lzo or uncompressed here	 
		dataFrame.write.parquet(<path to location>);	 
ORC FILE	READ	sqlContext.read.orc(<path to location>); //this results in a dataframe	 
	WRITE	df.write.mode(SaveMode.Overwrite).format("orc") .save(<path to location>)	orderdf.write.orc("/user/training/order/order_orc")
AVRO FILE	READ	import com.databricks.spark.avro._;	 
		sqlContext.read.avro(<path to location>); // this results in a data frame object	 
	WRITE	sqlContext.setConf("spark.sql.avro.compression.codec","snappy") //use snappy, deflate, uncompressed;	 
		dataFrame.write.avro(<path to location>);	 
JSON FILE	READ	sqlContext.read.json();	 
	WRITE	dataFrame.toJSON().saveAsTextFile(<path to location>,classOf[Compression Codec])	 
			


--------------------------

