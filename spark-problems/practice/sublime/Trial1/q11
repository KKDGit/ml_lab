q11
-----
spark.conf.set("spark.sql.shuffle.partitions", 10)
import org.apache.{spark => sprk}
import sprk.sql.functions._
import sprk.sql.expressions.Window


val cust = spark.read.option("sep","\t").csv("/user/cloudera/data_prep/retail_db/customers_part2_tab").toDF("customer_id","customer_fname","customer_state")

cust.show()

val fip1 = cust.where(substring('customer_fname,1,1) === "M").groupBy('customer_state).count()


val fip2 = cust.where("customer_fname like 'M%'").groupBy('customer_state).count()

fip1.select("customer_state","count").map(_.mkString(",")).write.text("/user/cloudera/DA_SPARK/question11/output")