/*
3. Using Spark Scala load data at /user/cloudera/problem1/orders and /user/cloudera/problem1/orders-items items as dataframes. 
4. Expected Intermediate Result: Order_Date , Order_status, total_orders, total_amount. In plain english, please find total orders and total amount per status per day. The result should be sorted by order date in descending, order status in ascending and total amount in descending and total orders in ascending. Aggregation should be done using below methods. However, sorting can be done using a dataframe or RDD. Perform aggregation in each of the following ways
a). Just by using Data Frames API - here order_date should be YYYY-MM-DD format
b). Using Spark SQL  - here order_date should be YYYY-MM-DD format
c). By using combineByKey function on RDDS -- No need of formatting order_date or total_amount
5. Store the result as parquet file into hdfs using gzip compression under folder
/user/cloudera/problem1/result4a-gzip
/user/cloudera/problem1/result4b-gzip
/user/cloudera/problem1/result4c-gzip
6. Store the result as parquet file into hdfs using snappy compression under folder
/user/cloudera/problem1/result4a-snappy
/user/cloudera/problem1/result4b-snappy
/user/cloudera/problem1/result4c-snappy
7. Store the result as CSV file into hdfs using No compression under folder
/user/cloudera/problem1/result4a-csv
/user/cloudera/problem1/result4b-csv
/user/cloudera/problem1/result4c-csv

8. create a mysql table named result and load data from /user/cloudera/problem1/result4a-csv to mysql table named result 
*/

/*
spark-shell \
--master yarn \
--conf spark.ui.port=4444 \
--num-executors 4 \
--executor-cores 2 \
--executor-memory 3G \
--packages graphframes:graphframes:0.6.0-spark2.3-s_2.11,mysql:mysql-connector-java:5.1.6,org.xerial:sqlite-jdbc:3.23.1,org.mongodb.spark:mongo-spark-connector_2.11:2.3.0,com.datastax.spark:spark-cassandra-connector_2.11:2.3.1,com.databricks:spark-avro_2.11:4.0.0
*/
//a.

import com.databricks.spark.avro._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val orders = spark.read.avro("/user/kranthidr/arun/problem1/orders")
val items = spark.read.avro("/user/kranthidr/arun/problem1/order-items")

//round(sum("order_item_subtotal"),2).as("("total_amount") -- can be used to round the column 
//countDistinct("order_id").as("total_orders") -- better method it seems

val joined = orders.join(items, 'order_id === 'order_item_order_id).
select(to_date(to_timestamp('order_date/1000)).as("order_date"),'order_status,'order_item_subtotal,'order_id).
groupBy("order_date","order_status").
agg(countDistinct("order_id").as("total_orders"),sum("order_item_subtotal").as("total_amount")).
orderBy('order_date.desc, 'order_status.asc,'total_amount.desc, 'total_orders.asc)

joined.write.mode("overwrite").option("compression", "gzip").parquet("/user/kranthidr/arun/problem1/result4a-gzip")
joined.write.mode("overwrite").option("compression", "snappy").parquet("/user/kranthidr/arun/problem1/result4a-snappy")
joined.write.mode("overwrite").csv("/user/kranthidr/arun/problem1/result4a-csv")

//b.
import com.databricks.spark.avro._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val orders1 = spark.read.avro("/user/kranthidr/arun/problem1/orders")
val items1 = spark.read.avro("/user/kranthidr/arun/problem1/order-items")

orders1.createOrReplaceTempView("orders1")
items1.createOrReplaceTempView("items1")


val joined1 = spark.sql("""
select order_date, order_status, count(distinct(order_id)) total_orders, sum(order_item_subtotal) total_amount 
from (
select order_id, cast(cast(order_date/1000 as timestamp) as date)  order_date , order_status, order_item_subtotal from orders1 join items1 on order_id = order_item_order_id 
) group by order_date, order_status
order by order_date desc, order_status asc,total_amount desc, total_orders asc
""")

joined1.write.mode("overwrite").option("compression", "gzip").parquet("/user/kranthidr/arun/problem1/result4b-gzip")
joined1.write.mode("overwrite").option("compression", "snappy").parquet("/user/kranthidr/arun/problem1/result4b-snappy")
joined1.write.mode("overwrite").csv("/user/kranthidr/arun/problem1/result4b-csv")

/*
val joinedOrderDataDF = orders1.join(items1,orders1("order_id")===items1("order_item_order_id"))
joinedOrderDataDF.createOrReplaceTempView("order_joined")

var sqlResult = sqlContext.sql("""
select to_date(from_unixtime(cast(order_date/1000 as bigint))) as order_formatted_date, order_status, 
cast(sum(order_item_subtotal) as DECIMAL (10,2)) as total_amount, count(distinct(order_id)) as total_orders 
from order_joined 
group by to_date(from_unixtime(cast(order_date/1000 as bigint))), order_status 
order by order_formatted_date desc,order_status,total_amount desc, total_orders
""")

*/
//c.

val joinedOrderDataDF = orders1.join(items1,orders1("order_id")===items1("order_item_order_id"))

val mapped1 = joinedOrderDataDF.map(x=> ((x(1).toString,x(3).toString),(x(8).toString.toFloat,x(0).toString)))

val mapped2 = mapped1.rdd.combineByKey(
(x:(Float, String)) => (x._1, Set(x._2)), //combiner to get initial values from each value
(x:(Float, Set[String]), y:(Float, String)) => (x._1 + y._1, x._2 + y._2), // merger for each partition
(x:(Float, Set[String]), y:(Float, Set[String])) => (x._1 + y._1, x._2 ++ y._2) // combiner for merged partitions
)

val mapped3 = mapped2.map(x=> (x._1._1,x._1._2,x._2._1,x._2._2.size))

var comByKeyResult = mapped3.toDF().orderBy(col("_1").desc,col("_2"),col("_3").desc,col("_4"))

comByKeyResult.show()








