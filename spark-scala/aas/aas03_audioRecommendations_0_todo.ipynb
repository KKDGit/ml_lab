{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4041)\" target=\"new_tab\">Spark UI: application_1533622723243_14476</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1533622723243_14476: Some(http://gw02.itversity.com:4041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://rm01.itversity.com:19288/proxy/application_1533622723243_14476\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(x=>if(x._2.contains(\"/proxy/\")){println(x._2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//spark.sparkContext.setCheckpointDir(\"hdfs:///tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base = hdfs:///user/kranthidr/dataSets/recommendations/audio_profiledata_06-May-2005/\n",
       "rawUserArtistData = [value: string]\n",
       "rawArtistData = [value: string]\n",
       "rawArtistAlias = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val base = \"hdfs:///user/kranthidr/dataSets/recommendations/audio_profiledata_06-May-2005/\"\n",
    "val rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "val rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "val rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24296858"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserArtistData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1848707"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArtistData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193027"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArtistAlias.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000002 1 55\n",
      "1000002 1000006 33\n",
      "1000002 1000007 8\n",
      "1000002 1000009 144\n",
      "1000002 1000010 314\n"
     ]
    }
   ],
   "source": [
    "rawUserArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userArtistDF = [user: int, artist: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[user: int, artist: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userArtistDF = rawUserArtistData.map { line =>\n",
    "      val Array(user, artist, _*) = line.split(' ')\n",
    "      (user.toInt, artist.toInt)\n",
    "    }.toDF(\"user\", \"artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   user| artist|\n",
      "+-------+-------+\n",
      "|1000002|      1|\n",
      "|1000002|1000006|\n",
      "|1000002|1000007|\n",
      "|1000002|1000009|\n",
      "|1000002|1000010|\n",
      "|1000002|1000013|\n",
      "+-------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   user|count|\n",
      "+-------+-----+\n",
      "|2030067| 6836|\n",
      "|1024631| 6188|\n",
      "|1059334| 5864|\n",
      "|2016026| 5806|\n",
      "|2023686| 4863|\n",
      "|1045464| 4796|\n",
      "|1004584| 4709|\n",
      "|2010008| 4512|\n",
      "|2232770| 4115|\n",
      "|1006093| 4019|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.groupBy(\"user\").count().orderBy($\"count\".desc).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   user|count|\n",
      "+-------+-----+\n",
      "|2042404|    1|\n",
      "|2077194|    1|\n",
      "|2114054|    1|\n",
      "|1054207|    1|\n",
      "|2315849|    1|\n",
      "|2337539|    1|\n",
      "|2389464|    1|\n",
      "|1008233|    1|\n",
      "|1034317|    1|\n",
      "|1051364|    1|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.groupBy(\"user\").count().orderBy($\"count\".asc).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134999\t06Crazy Life\n",
      "6821360\tPang Nakarin\n",
      "10113088\tTerfel, Bartoli- Mozart: Don\n",
      "10151459\tThe Flaming Sidebur\n",
      "6826647\tBodenstandig 3000\n"
     ]
    }
   ],
   "source": [
    "rawArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092764\t1000311\n",
      "1095122\t1000557\n",
      "6708070\t1007267\n",
      "10088054\t1042317\n",
      "1195917\t1042317\n"
     ]
    }
   ],
   "source": [
    "rawArtistAlias.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildArtistByID: (rawArtistData: org.apache.spark.sql.Dataset[String])org.apache.spark.sql.DataFrame\n",
       "buildArtistAlias: (rawArtistAlias: org.apache.spark.sql.Dataset[String])scala.collection.Map[Int,Int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " def buildArtistByID(rawArtistData: Dataset[String]): DataFrame = {\n",
    "    rawArtistData.flatMap { line =>\n",
    "      val (id, name) = line.span(_ != '\\t')\n",
    "      if (name.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        try {\n",
    "          Some((id.toInt, name.trim))\n",
    "        } catch {\n",
    "          case _: NumberFormatException => None\n",
    "        }\n",
    "      }\n",
    "    }.toDF(\"id\", \"name\")\n",
    "  }\n",
    "\n",
    "  def buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] = {\n",
    "    rawArtistAlias.flatMap { line =>\n",
    "      val Array(artist, alias) = line.split('\\t')\n",
    "      if (artist.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        Some((artist.toInt, alias.toInt))\n",
    "      }\n",
    "    }.collect().toMap\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artistByID = [id: int, name: string]\n",
       "artistAlias = Map(1208690 -> 1003926, 2012757 -> 4569, 6949139 -> 1085752, 1109727 -> 1239120, 6772751 -> 1244705, 2070533 -> 1021544, 1157679 -> 2194, 9969617 -> 5630, 2034496 -> 1116214, 6764342 -> 40, 1272489 -> 1278238, 2108744 -> 1009267, 10349857 -> 1000052, 2145319 -> 1020463, 2126338 -> 2717, 10165456 -> 1001169, 6779368 -> 1239506, 10278137 -> 1001523, 9939075 -> 1329390, 2037201 -> 1274155, 1248585 -> 2885, 1106945 -> 1399, 6811322 -> 1019016, 9978396 -> 1784, 6676961 -> 1086433, 2117821 -> 2611, 6863616 -> 1277013, 6895480 -> 1000993, 6831632 -> 1246136, 1001719 -> 1009727, 10135633 -> 4250, 7029291 -> 1034635, 6967939 -> 1002734, 6864694 -> 1017311, 1237279 -> 1029752, 6793956...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "val artistByID = buildArtistByID(rawArtistData)\n",
    "val artistAlias = buildArtistAlias(rawArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "badID = 1208690\n",
       "goodID = 1003926\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1003926"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (badID, goodID) = artistAlias.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|     id|            name|\n",
      "+-------+----------------+\n",
      "|1208690|Collective Souls|\n",
      "|1003926| Collective Soul|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter($\"id\" isin (badID, goodID)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildCounts: (rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def buildCounts(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "    rawUserArtistData.map { line =>\n",
    "      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "      (userID, finalArtistID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bArtistAlias = Broadcast(36)\n",
       "trainData = [user: int, artist: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[user: int, artist: int ... 1 more field]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "val trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|   user| artist|count|\n",
      "+-------+-------+-----+\n",
      "|1000002|      1|   55|\n",
      "|1000002|1000006|   33|\n",
      "|1000002|1000007|    8|\n",
      "|1000002|1000009|  144|\n",
      "|1000002|1000010|  314|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = als_9b43b41b34ce\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "als_9b43b41b34ce"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val model = new ALS().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setImplicitPrefs(true).\n",
    "      setRank(10).\n",
    "      setRegParam(0.01).\n",
    "      setAlpha(1.0).\n",
    "      setMaxIter(5).\n",
    "      setUserCol(\"user\").\n",
    "      setItemCol(\"artist\").\n",
    "      setRatingCol(\"count\").\n",
    "      setPredictionCol(\"prediction\").\n",
    "      fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                                                |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "|90 |[0.23245662, -0.089350305, 0.4406498, -0.52033013, 1.1043696, 0.15526487, 0.34216267, 0.40751842, 0.1901746, -0.6059311]|\n",
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.userFactors.show(1,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|[0.23245662, -0.089350305, 0.4406498, -0.52033013, 1.1043696, 0.15526487, 0.34216267, 0.40751842, 0.1901746, -0.6059311]|\n",
      "|[0.2145854, 0.08403039, -0.0794857, 0.2887465, 0.18284331, 0.10450412, -0.0195537, 0.10427659, -0.15771231, -0.06480175]|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " model.userFactors.select(\"features\").show(2,truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toRecommend = [artist: int, user: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[artist: int, user: int]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val toRecommend = model.itemFactors.\n",
    "      select($\"id\".as(\"artist\")).\n",
    "      withColumn(\"user\", lit(userID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568126"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toRecommend.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|artist|user   |\n",
      "+------+-------+\n",
      "|30    |2093760|\n",
      "|40    |2093760|\n",
      "|50    |2093760|\n",
      "|70    |2093760|\n",
      "|90    |2093760|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toRecommend.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Detected cartesian product for LEFT OUTER join between logical plans\n",
       "Project [_1#283 AS artist#407]\n",
       "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#283, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#284]\n",
       "   +- ExternalRDD [obj#282]\n",
       "and\n",
       "Project [_2#274 AS features#277]\n",
       "+- Filter (UDF(2093760) = _1#273)\n",
       "   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#273, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#274]\n",
       "      +- ExternalRDD [obj#272]\n",
       "Join condition is missing or trivial.\n",
       "Use the CROSS JOIN syntax to allow cartesian products between these relations.;\n",
       "StackTrace: Project [_1#283 AS artist#407]\n",
       "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#283, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#284]\n",
       "   +- ExternalRDD [obj#282]\n",
       "and\n",
       "Project [_2#274 AS features#277]\n",
       "+- Filter (UDF(2093760) = _1#273)\n",
       "   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#273, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#274]\n",
       "      +- ExternalRDD [obj#272]\n",
       "Join condition is missing or trivial.\n",
       "Use the CROSS JOIN syntax to allow cartesian products between these relations.;\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n",
       "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:725)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(toRecommend).show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeRecommendations: (model: org.apache.spark.ml.recommendation.ALSModel, userID: Int, howMany: Int)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\n",
    "    val toRecommend = model.itemFactors.\n",
    "      select($\"id\".as(\"artist\")).\n",
    "      withColumn(\"user\", lit(userID))\n",
    "    \n",
    "    model.transform(toRecommend).\n",
    "      select(\"artist\", \"prediction\").\n",
    "      orderBy($\"prediction\".desc).\n",
    "      limit(howMany)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID = 2093760\n",
       "existingArtistIDs = Array(1180, 1255340, 378, 813, 942)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1180, 1255340, 378, 813, 942]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userID = 2093760\n",
    "\n",
    "val existingArtistIDs = trainData.\n",
    "      filter($\"user\" === userID).\n",
    "      select(\"artist\").as[Int].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|     id|           name|\n",
      "+-------+---------------+\n",
      "|   1180|     David Gray|\n",
      "|    378|  Blackalicious|\n",
      "|    813|     Jurassic 5|\n",
      "|1255340|The Saw Doctors|\n",
      "|    942|         Xzibit|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Detected cartesian product for LEFT OUTER join between logical plans\n",
       "Project [_1#283 AS artist#357]\n",
       "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#283, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#284]\n",
       "   +- ExternalRDD [obj#282]\n",
       "and\n",
       "Project [_2#274 AS features#277]\n",
       "+- Filter (UDF(2093760) = _1#273)\n",
       "   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#273, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#274]\n",
       "      +- ExternalRDD [obj#272]\n",
       "Join condition is missing or trivial.\n",
       "Use the CROSS JOIN syntax to allow cartesian products between these relations.;\n",
       "StackTrace: Project [_1#283 AS artist#357]\n",
       "+- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#283, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#284]\n",
       "   +- ExternalRDD [obj#282]\n",
       "and\n",
       "Project [_2#274 AS features#277]\n",
       "+- Filter (UDF(2093760) = _1#273)\n",
       "   +- SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#273, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, assertnotnull(input[0, scala.Tuple2, true])._2, true, false) AS _2#274]\n",
       "      +- ExternalRDD [obj#272]\n",
       "Join condition is missing or trivial.\n",
       "Use the CROSS JOIN syntax to allow cartesian products between these relations.;\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\n",
       "  at org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n",
       "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "topRecommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "\n",
    "artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "cvData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(10).setRegParam(0.01).\n",
    "          setAlpha(1.0).setMaxIter(5).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "areaUnderCurve(cvData, bAllArtistIDs, model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame = {\n",
    "    val listenCounts = train.groupBy(\"artist\").\n",
    "      agg(sum(\"count\").as(\"prediction\")).\n",
    "      select(\"artist\", \"prediction\")\n",
    "    allData.\n",
    "      join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "      select(\"user\", \"artist\", \"prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "println(mostListenedAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val evaluations =\n",
    "      for (rank     <- Seq(5,  30);\n",
    "           regParam <- Seq(1.0, 0.0001);\n",
    "           alpha    <- Seq(1.0, 40.0))\n",
    "      yield {\n",
    "        val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(rank).setRegParam(regParam).\n",
    "          setAlpha(alpha).setMaxIter(20).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)\n",
    "\n",
    "        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "\n",
    "        (auc, (rank, regParam, alpha))\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluations.sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val someUsers = allData.select(\"user\").as[Int].distinct().take(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val someRecommendations = someUsers.map(userID => (userID, makeRecommendations(model, userID, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "someRecommendations.foreach{\n",
    "    \n",
    "case(userID, recsDF) => \n",
    "    \n",
    "val recommendedArtists = recsDF.select(\"artist\").as[Int].collect()\n",
    "    \n",
    "printn(s\"$userID -> ${recommendedArtists.mkString(\", \")}\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
