{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4051)\" target=\"new_tab\">Spark UI: application_1533622723243_14500</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1533622723243_14500: Some(http://gw02.itversity.com:4051)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://rm01.itversity.com:19288/proxy/application_1533622723243_14500\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(x=>if(x._2.contains(\"/proxy/\")){println(x._2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//spark.sparkContext.setCheckpointDir(\"hdfs:///tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base = hdfs:///user/kranthidr/dataSets/recommendations/audio_profiledata_06-May-2005/\n",
       "rawUserArtistData = [value: string]\n",
       "rawArtistData = [value: string]\n",
       "rawArtistAlias = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val base = \"hdfs:///user/kranthidr/dataSets/recommendations/audio_profiledata_06-May-2005/\"\n",
    "val rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "val rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "val rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24296858"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserArtistData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1848707"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArtistData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193027"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArtistAlias.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000002 1 55\n",
      "1000002 1000006 33\n",
      "1000002 1000007 8\n",
      "1000002 1000009 144\n",
      "1000002 1000010 314\n"
     ]
    }
   ],
   "source": [
    "rawUserArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134999\t06Crazy Life\n",
      "6821360\tPang Nakarin\n",
      "10113088\tTerfel, Bartoli- Mozart: Don\n",
      "10151459\tThe Flaming Sidebur\n",
      "6826647\tBodenstandig 3000\n"
     ]
    }
   ],
   "source": [
    "rawArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092764\t1000311\n",
      "1095122\t1000557\n",
      "6708070\t1007267\n",
      "10088054\t1042317\n",
      "1195917\t1042317\n"
     ]
    }
   ],
   "source": [
    "rawArtistAlias.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildCounts: (rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def buildCounts(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "    rawUserArtistData.map { line =>\n",
    "      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "      (userID, finalArtistID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "makeRecommendations: (model: org.apache.spark.ml.recommendation.ALSModel, userID: Int, howMany: Int)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\n",
    "    val toRecommend = model.itemFactors.\n",
    "      select($\"id\".as(\"artist\")).\n",
    "      withColumn(\"user\", lit(userID))\n",
    "    \n",
    "    model.transform(toRecommend).\n",
    "      select(\"artist\", \"prediction\").\n",
    "      orderBy($\"prediction\".desc).\n",
    "      limit(howMany)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "cvData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(10).setRegParam(0.01).\n",
    "          setAlpha(1.0).setMaxIter(5).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "areaUnderCurve(cvData, bAllArtistIDs, model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame = {\n",
    "    val listenCounts = train.groupBy(\"artist\").\n",
    "      agg(sum(\"count\").as(\"prediction\")).\n",
    "      select(\"artist\", \"prediction\")\n",
    "    allData.\n",
    "      join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "      select(\"user\", \"artist\", \"prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "println(mostListenedAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val evaluations =\n",
    "      for (rank     <- Seq(5,  30);\n",
    "           regParam <- Seq(1.0, 0.0001);\n",
    "           alpha    <- Seq(1.0, 40.0))\n",
    "      yield {\n",
    "        val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(rank).setRegParam(regParam).\n",
    "          setAlpha(alpha).setMaxIter(20).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)\n",
    "\n",
    "        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "\n",
    "        (auc, (rank, regParam, alpha))\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluations.sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val someUsers = allData.select(\"user\").as[Int].distinct().take(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val someRecommendations = someUsers.map(userID => (userID, makeRecommendations(model, userID, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "someRecommendations.foreach{\n",
    "    \n",
    "case(userID, recsDF) => \n",
    "    \n",
    "val recommendedArtists = recsDF.select(\"artist\").as[Int].collect()\n",
    "    \n",
    "printn(s\"$userID -> ${recommendedArtists.mkString(\", \")}\")\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
