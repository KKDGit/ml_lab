{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform,os, sys\n",
    "os_name = platform.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os_name == \"Windows\":\n",
    "    findspark.init()\n",
    "    img_dir = \"D:/Learn/GitRepos/Spark-The-Definitive-Guide/data/deep-learning-images\"\n",
    "    temp_loc = \"D:/temp\"\n",
    "    master = \"local[*]\"\n",
    "    driver_memory = \"24g\"\n",
    "else:\n",
    "    spark_home = \"/usr/hdp/current/spark2-client\"\n",
    "    findspark.init(spark_home)\n",
    "    hdfs_home = \"/user/\" + os.getenv(\"HOME\").split(\"/\")[2]\n",
    "    img_dir = \"/home/kranthidr/data/sdl-flowers/\"\n",
    "    temp_loc = os.getenv(\"HOME\") + \"/temp\"\n",
    "    master = \"yarn\"\n",
    "    driver_memory = \"2g\"\n",
    "    #sys.path.append(os.path.expanduser('hdfs:///user/kranthidr/pythonpath/sparkdl.zip'))\n",
    "    sys.path.append(os.path.expanduser('~/pythonpath/sparkdl.zip'))\n",
    "    sys.path.append(os.path.expanduser('~/pythonpath/tensorframes.zip'))\n",
    "    sys.path.append(os.path.expanduser('~/pythonpath/tensorflow.zip'))\n",
    "    sys.path.append(os.path.expanduser(\"~/.local/lib/python2.7/site-packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Spark\\\\spark-hadoop\\\\python',\n",
       " 'C:\\\\Spark\\\\spark-hadoop\\\\python\\\\lib\\\\py4j-0.10.6-src.zip',\n",
       " '',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\scripts\\\\python36.zip',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\DLLs',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\lib',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\scripts',\n",
       " 'c:\\\\users\\\\kranthikumar\\\\anaconda3\\\\Lib',\n",
       " 'c:\\\\users\\\\kranthikumar\\\\anaconda3\\\\DLLs',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\lib\\\\site-packages',\n",
       " 'D:\\\\Learn\\\\PythonProjects\\\\ml_lab\\\\pyspark-lab',\n",
       " 'd:\\\\learn\\\\.virtualenvs\\\\pyspark-lab\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\KranthiKumar\\\\.ipython']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-hadoop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Learn/GitRepos/Spark-The-Definitive-Guide/data/deep-learning-images'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/temp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(master)\\\n",
    ".config(\"spark.driver.memory\", driver_memory)\\\n",
    ".config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11\")\\\n",
    ".appName(\"ch31_DeepLearningTuning\").getOrCreate()\n",
    "\n",
    "#.config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11,databricks:tensorframes:0.5.0-s_2.11\")\\\n",
    "# .config(\"spark.driver.memory\", \"8g\")\\\n",
    "# .config(\"spark.executor.memory\", \"3g\")\\\n",
    "# .config(\"spark.executor.cores\", \"4\")\\\n",
    "# .config(\"spark.executor.instances\", \"10\")\\\n",
    "\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This file is sourced when running various Spark programs.\n",
    "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
    "\n",
    "# Options read in YARN client mode\n",
    "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n",
    "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n",
    "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n",
    "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n",
    "#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\n",
    "#SPARK_YARN_QUEUE=\"default\" #The hadoop queue to use for allocation requests (Default: default)\n",
    "#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\n",
    "#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sc._conf.getAll():\n",
    "    if \"/proxy/\" in x[1]:\n",
    "        print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINDOWS-JVACTDB:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch31_DeepLearningTuning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21f5a8bfac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights= None, classes=2)\n",
    "model.save(temp_loc + \"/model-full-tmp.h5\")\n",
    "\n",
    "# Signature: InceptionV3(include_top=True, weights='imagenet', \n",
    "#                        input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "# Docstring:\n",
    "# Instantiates the Inception v3 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from os import listdir\n",
    "def absoluteFilePaths(directory):\n",
    "   for dirpath,_,filenames in os.walk(directory):\n",
    "       for f in filenames:\n",
    "           yield os.path.abspath(os.path.join(dirpath, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object absoluteFilePaths at 0x0000021F68DDF0F8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absoluteFilePaths(img_dir + \"/tulips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips_files = [str(f) for f in absoluteFilePaths(img_dir + \"/tulips\")]  # make \"local\" file paths for images\n",
    "daisy_files = [str(f) for f in absoluteFilePaths(img_dir + \"/daisy\")]  # make \"local\" file paths for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\tulips\\\\3498663243_42b39b4185_m.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tulips_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\daisy\\\\10555815624_dc211569b0.jpg']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daisy_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.linalg as spla\n",
    "import pyspark.sql.types as sptyp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTrainImageUriandLabels(image_uris, label, cardinality):\n",
    "  # Create image categorical labels (integer IDs)\n",
    "  local_rows = []\n",
    "  for uri in image_uris:\n",
    "    label_inds = np.zeros(cardinality)\n",
    "    label_inds[label] = 1.0\n",
    "    one_hot_vec = spla.Vectors.dense(label_inds.tolist())\n",
    "    _row_struct = {\"uri\": uri, \"one_hot_label\": one_hot_vec, \"label\": float(label)}\n",
    "    row = sptyp.Row(**_row_struct)\n",
    "    local_rows.append(row)\n",
    "\n",
    "  image_uri_df = spark.createDataFrame(local_rows)\n",
    "  return image_uri_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cardinality = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips_uri_df = CreateTrainImageUriandLabels(tulips_files,1,label_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "daisy_uri_df = CreateTrainImageUriandLabels(daisy_files,0,label_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3498663243_42b39b4185_m.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3501996215_1c6d1a3386_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tulips_uri_df.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |[1.0,0.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\daisy\\10555815624_dc211569b0.jpg  |\n",
      "|0.0  |[1.0,0.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\daisy\\10555826524_423eb8bf71_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daisy_uri_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tulips_train, tulips_test, _ = tulips_uri_df.randomSplit([0.005, 0.005, 0.99]) \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "tulips_train, tulips_test = tulips_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daisy_train, daisy_test, _ = daisy_uri_df.randomSplit([0.005, 0.005, 0.99])    \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "daisy_train, daisy_test = daisy_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tulips_train.unionAll(daisy_train)\n",
    "test_df = tulips_test.unionAll(daisy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, one_hot_label: vector, uri: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.repartition(100).cache()\n",
    "test_df.repartition(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3498663243_42b39b4185_m.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3501996215_1c6d1a3386_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502085373_edc2c36992_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502615974_ef4bd13202_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502685880_f026400dce_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3511104954_54eace015c_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3, False)\n",
    "test_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sparkdl.estimators.keras_image_file_estimator import KerasImageFileEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_uri(local_uri):\n",
    "  img = (PIL.Image.open(local_uri).convert('RGB').resize((299, 299), PIL.Image.ANTIALIAS))\n",
    "  img_arr = np.array(img).astype(np.float32)\n",
    "  img_tnsr = preprocess_input(img_arr[np.newaxis, :])\n",
    "  return img_tnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\tulips\\\\3498663243_42b39b4185_m.jpg'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select(\"uri\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = load_image_from_uri(train_df.select(\"uri\").first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 299, 299, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# This ensure that each of the paritions has a small size.\n",
    "# train_df = train_df.repartition(100)\n",
    "# test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   24|\n",
      "|  1.0|   25|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimatorT = KerasImageFileEstimator( inputCol=\"uri\",\n",
    "#                                      outputCol=\"prediction\",\n",
    "#                                      labelCol=\"one_hot_label\",\n",
    "#                                      imageLoader=load_image_from_uri,\n",
    "#                                      kerasOptimizer='adam',\n",
    "#                                      kerasLoss='categorical_crossentropy', # why categorical_crossentropy\n",
    "#                                      modelFile=temp_loc+\"/model-full-tmp.h5\", # local file path for model\n",
    "#                                      kerasFitParams = {\"batch_size\": 32, \"verbose\": 0}\n",
    "#                                    ) \n",
    "\n",
    "#     transformers = estimator.fit(image_dataset)\n",
    "# Init docstring:\n",
    "# __init__(self, inputCol=None, outputCol=None, \n",
    "#          outputMode=\"vector\", labelCol=None,\n",
    "#          modelFile=None, imageLoader=None, \n",
    "#          kerasOptimizer=None, kerasLoss=None,\n",
    "#          kerasFitParams=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fittedT = estimatorT.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasImageFileEstimator( inputCol=\"uri\",\n",
    "                                     outputCol=\"prediction\",\n",
    "                                     labelCol=\"one_hot_label\",\n",
    "                                     imageLoader=load_image_from_uri,\n",
    "                                     kerasOptimizer='adam',\n",
    "                                     kerasLoss='categorical_crossentropy', # why categorical_crossentropy\n",
    "                                     modelFile=temp_loc+\"/model-full-tmp.h5\" # local file path for model\n",
    "                                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "  ParamGridBuilder()\n",
    "  .addGrid(estimator.kerasFitParams, [\n",
    "                                      {\"batch_size\": 64, \"verbose\": 0}])\n",
    "  .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = (\n",
    "#   ParamGridBuilder()\n",
    "#   .addGrid(estimator.kerasFitParams, [{\"batch_size\": 32, \"verbose\": 0},\n",
    "#                                       {\"batch_size\": 64, \"verbose\": 0}])\n",
    "#   .build()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\" )\n",
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=bc, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 378 variables.\n",
      "Converted 378 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/tmp\\\\tmpj9rq2nowtensorframes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-5c81b141f4da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kranthikumar\\anaconda3\\Lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kranthikumar\\anaconda3\\Lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\transformers\\keras_image.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              \u001b[0moutputTensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputTensorName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                                              outputMode=self.getOrDefault(self.outputMode))\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loadedImageCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\transformers\\tf_image.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    145\u001b[0m                                  \u001b[1;34m\"width\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"__sdl_image_width\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                                  \u001b[1;34m\"num_channels\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"__sdl_image_nchannels\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                                  \"image_buffer\": \"__sdl_image_data\"})\n\u001b[0m\u001b[0;32m    148\u001b[0m                 .drop(\"__sdl_image_height\", \"__sdl_image_width\", \"__sdl_image_nchannels\",\n\u001b[0;32m    149\u001b[0m                       \"__sdl_image_data\")\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_tensorframes-0.5.0-s_2.11.jar\\tensorframes\\core.py\u001b[0m in \u001b[0;36mmap_rows\u001b[1;34m(fetches, dframe, feed_dict, initial_variables)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_map_pd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmap_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_initial_variables_default\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_tensorframes-0.5.0-s_2.11.jar\\tensorframes\\core.py\u001b[0m in \u001b[0;36m_map\u001b[1;34m(fetches, dframe, feed_dict, block, trim, initial_variables)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_java_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0m_add_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mph_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_add_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0m_add_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mph_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_tensorframes-0.5.0-s_2.11.jar\\tensorframes\\core.py\u001b[0m in \u001b[0;36m_add_graph\u001b[1;34m(graph, builder, use_file)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# TODO: remove the dir and honor the existing one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tensorframes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/tmp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"proto.pb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/proto.pb\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\learn\\.virtualenvs\\pyspark-lab\\lib\\tempfile.py\u001b[0m in \u001b[0;36mmkdtemp\u001b[1;34m(suffix, prefix, dir)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0o700\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mcontinue\u001b[0m    \u001b[1;31m# try again\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/tmp\\\\tmpj9rq2nowtensorframes'"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(train_df)\n",
    "\n",
    "# ~\\AppData\\Local\\Temp\\spark-5780b8a4-c5ad-43ba-b92a-dfde3f4a482c\n",
    "# \\userFiles-2c99e8b3-645f-4bf2-bfc6-069f994f8c2c\\databricks_tensorframes-0.5.0-s_2.11.jar\n",
    "# \\tensorframes\\core.py in _add_graph(graph, builder, use_file)\n",
    "#      59     if use_file:\n",
    "#      60         # TODO: remove the dir and honor the existing one\n",
    "# ---> 61         d = tempfile.mkdtemp(\"tensorframes\", dir=\"/tmp\")\n",
    "#      62         tf.train.write_graph(graph, d, \"proto.pb\", False)\n",
    "#      63         fname = d + \"/proto.pb\"\n",
    "\n",
    "# d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\tempfile.py in mkdtemp(suffix, prefix, dir)\n",
    "#     366         file = _os.path.join(dir, prefix + name + suffix)\n",
    "#     367         try:\n",
    "# --> 368             _os.mkdir(file, 0o700)\n",
    "#     369         except FileExistsError:\n",
    "#     370             continue    # try again\n",
    "\n",
    "# FileNotFoundError: [WinError 3] The system cannot find the path specified: '/tmp\\\\tmpj9rq2nowtensorframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel = cv.fit(train_df)\n",
    "\n",
    "# #\n",
    "# File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 137, in standardize_input_data\n",
    "#     str(data_shape))\n",
    "# ValueError: Error when checking target: expected predictions to have shape (1000,) but got array with shape (2,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Activation, Dense, Flatten\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(299, 299, 3)))\n",
    "# model.add(Dense(2))\n",
    "# model.add(Activation(\"softmax\"))\n",
    "# model.save(temp_loc + \"/model-full-tmp1.h5\")  # saves to the local filesystem\n",
    "\n",
    "#DURING FIT inKerasImageFileEstimator\n",
    "\n",
    "# Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    "# : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, \n",
    "#             most recent failure: Lost task 0.0 in stage 37.0 (TID 529, localhost, executor driver): \n",
    "#                     org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
    "#   File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n",
    "#     return fn(*args)\n",
    "#   File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n",
    "#     target_list, status, run_metadata)\n",
    "#   File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n",
    "#     c_api.TF_GetCode(self.status.status))\n",
    "# tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), \n",
    "#         b.shape=(268203, 2), m=32, n=2, k=268203\n",
    "# \t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, \n",
    "#                                      _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]\n",
    "#        (flatten_1/Reshape, dense_1/kernel/read)]]\n",
    "# \t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, \n",
    "#                                          recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", \n",
    "                                         \n",
    "#                                          send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", \n",
    "#                                          send_device_incarnation=1, \n",
    "#                                          tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, \n",
    "#                                          _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-lab",
   "language": "python",
   "name": "pyspark-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
