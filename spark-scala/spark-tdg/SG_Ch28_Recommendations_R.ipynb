{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4040)\" target=\"new_tab\">Spark UI: application_1533622723243_11683</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1533622723243_11683: Some(http://gw02.itversity.com:4040)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ratings = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "val ratings = spark.read.textFile(\"/user/kranthidr/dataSets/spark-guide/data/sample_movielens_ratings.txt\")\n",
    "  .selectExpr(\"split(value , '::') as col\")\n",
    "  .selectExpr(\n",
    "    \"cast(col[0] as int) as userId\",\n",
    "    \"cast(col[1] as int) as movieId\",\n",
    "    \"cast(col[2] as float) as rating\",\n",
    "    \"cast(col[3] as long) as timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training = [userId: int, movieId: int ... 2 more fields]\n",
       "test = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als = als_28cbfdbb90a3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "als_28cbfdbb90a3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val als = new ALS()\n",
    ".setColdStartStrategy(\"drop\")\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"userId\")\n",
    "  .setItemCol(\"movieId\")\n",
    "  .setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: nan,drop. (default: nan, current: drop)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: false)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 10, current: 10)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: false)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0) (default: 0.1, current: 0.01)\n",
      "seed: random seed (default: 1994790107)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "println(als.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alsModel = als_28cbfdbb90a3\n",
       "predictions = [userId: int, movieId: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val alsModel = als.fit(training)\n",
    "val predictions = alsModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    28|[76, 6.5622416]|\n",
      "|    28| [29, 5.886418]|\n",
      "|    28| [51, 4.975867]|\n",
      "|    28| [81, 4.947612]|\n",
      "|    28| [24, 4.660326]|\n",
      "|    28| [53, 4.616293]|\n",
      "|    28|[72, 4.5752296]|\n",
      "|    28| [4, 4.2370954]|\n",
      "|    28|[49, 3.9188836]|\n",
      "|    28|[55, 3.7959673]|\n",
      "|    26| [46, 7.144871]|\n",
      "|    26|[30, 6.4189463]|\n",
      "|    26| [94, 6.096276]|\n",
      "|    26| [74, 5.691221]|\n",
      "|    26|[22, 5.2658143]|\n",
      "|    26|[88, 5.2166443]|\n",
      "|    26| [23, 5.041458]|\n",
      "|    26| [32, 4.800029]|\n",
      "|    26|[24, 4.7965646]|\n",
      "|    26|  [7, 4.787397]|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "alsModel.recommendForAllUsers(10)\n",
    "  .selectExpr(\"userId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     31|[12, 3.7455027]|\n",
      "|     31|[15, 3.0927289]|\n",
      "|     31|[14, 2.9269235]|\n",
      "|     31| [6, 2.9064713]|\n",
      "|     31| [7, 2.6342008]|\n",
      "|     31|[25, 2.2124536]|\n",
      "|     31|[16, 2.2068238]|\n",
      "|     31| [8, 1.6802181]|\n",
      "|     31|[27, 1.4390929]|\n",
      "|     31| [2, 1.2938327]|\n",
      "|     85|[16, 5.0699587]|\n",
      "|     85| [8, 4.8067217]|\n",
      "|     85|[14, 4.6430855]|\n",
      "|     85|  [7, 4.333009]|\n",
      "|     85|[17, 3.6258996]|\n",
      "|     85| [1, 3.0400844]|\n",
      "|     85| [6, 2.9789066]|\n",
      "|     85|[19, 2.3474717]|\n",
      "|     85|  [3, 2.076093]|\n",
      "|     85|[20, 2.0634823]|\n",
      "+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllItems(10)\n",
    "  .selectExpr(\"movieId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.8194261925717494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = regEval_a6e4bc270eb1\n",
       "rmse = 1.8194261925717494\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8194261925717494"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setLabelCol(\"rating\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root-mean-square error = $rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regComparison = MapPartitionsRDD[309] at map at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[309] at map at <console>:42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.mllib.evaluation.{\n",
    "  RankingMetrics,\n",
    "  RegressionMetrics}\n",
    "val regComparison = predictions.select(\"rating\", \"prediction\")\n",
    "  .rdd.map(x => (x.getFloat(0).toDouble,x.getFloat(1).toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metrics = org.apache.spark.mllib.evaluation.RegressionMetrics@7a93d619\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.RegressionMetrics@7a93d619"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new RegressionMetrics(regComparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.819426192571749"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3804192696533988\n",
      "1.347436827484674\n",
      "3.310311670216131\n",
      "-0.2792667010567924\n",
      "1.819426192571749\n"
     ]
    }
   ],
   "source": [
    "println(metrics.explainedVariance)\n",
    "println(metrics.meanAbsoluteError)\n",
    "println(metrics.meanSquaredError)\n",
    "println(metrics.r2)\n",
    "println(metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perUserActual = [userId: int, movies: array<int>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movies: array<int>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\n",
    "import org.apache.spark.sql.functions.{col, expr}\n",
    "val perUserActual = predictions\n",
    "  .where(\"rating > 2.5\")\n",
    "  .groupBy(\"userId\")\n",
    "  .agg(expr(\"collect_set(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perUserPredictions = [userId: int, movies: array<int>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movies: array<int>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val perUserPredictions = predictions\n",
    "  .orderBy(col(\"userId\"), col(\"prediction\").desc)\n",
    "  .groupBy(\"userId\")\n",
    "  .agg(expr(\"collect_list(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perUserActualvPred = [_1: array<int>, _2: array<int>]\n",
       "ranks = org.apache.spark.mllib.evaluation.RankingMetrics@2a25c1d7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.evaluation.RankingMetrics@2a25c1d7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val perUserActualvPred = perUserActual.join(perUserPredictions, Seq(\"userId\"))\n",
    "  .map(row => (\n",
    "    row(1).asInstanceOf[Seq[Integer]].toArray,\n",
    "    row(2).asInstanceOf[Seq[Integer]].toArray.take(15)\n",
    "  ))\n",
    "val ranks = new RankingMetrics(perUserActualvPred.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23934657934657927"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "ranks.meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4666666666666668"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.precisionAt(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
