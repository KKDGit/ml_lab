{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4044)\" target=\"new_tab\">Spark UI: application_1540458187951_75007</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1540458187951_75007: Some(http://gw02.itversity.com:4044)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://rm01.itversity.com:19288/proxy/application_1540458187951_75007"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getType: (o: Any)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os_name = Linux\n",
       "hdfs_home = /user/kranthidr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /user/kranthidr/samples/auto_data.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/samples/auto_data.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = hdfs_home + \"/samples/auto_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_data1 = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_data1 = spark.read.text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|18,8,307,130,3504...|\n",
      "|15,8,350,165,3693...|\n",
      "|18,8,318,150,3436...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>18,8,307,130,3504,12,70,1</td></tr>\n",
       "<tr><td>15,8,350,165,3693,11.5,70,1</td></tr>\n",
       "<tr><td>18,8,318,150,3436,11,70,1</td></tr>\n",
       "<tr><td>16,8,304,150,3433,12,70,1</td></tr>\n",
       "<tr><td>17,8,302,140,3449,10.5,70,1</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------------------------+\n",
       "| 18,8,307,130,3504,12,70,1   |\n",
       "| 15,8,350,165,3693,11.5,70,1 |\n",
       "| 18,8,318,150,3436,11,70,1   |\n",
       "| 16,8,304,150,3433,12,70,1   |\n",
       "| 17,8,302,140,3449,10.5,70,1 |\n",
       "+-----------------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.Dataset\n",
      "org.apache.spark.sql.Row[]\n",
      "org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n"
     ]
    }
   ],
   "source": [
    "println(getType(raw_data1))\n",
    "println(getType(raw_data1.take(1)))\n",
    "println(getType(raw_data1.take(1)(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_data2 = /user/kranthidr/samples/auto_data.csv MapPartitionsRDD[17] at textFile at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/samples/auto_data.csv MapPartitionsRDD[17] at textFile at <console>:31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_data2 = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,8,307,130,3504,12,70,1, 15,8,350,165,3693,11.5,70,1, 18,8,318,150,3436,11,70,1, 16,8,304,150,3433,12,70,1, 17,8,302,140,3449,10.5,70,1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.rdd.MapPartitionsRDD\n",
      "java.lang.String[]\n",
      "java.lang.String\n"
     ]
    }
   ],
   "source": [
    "println(getType(raw_data2))\n",
    "println(getType(raw_data2.take(1)))\n",
    "println(getType(raw_data2.take(1)(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|18,8,307,130,3504...|\n",
      "|15,8,350,165,3693...|\n",
      "|18,8,318,150,3436...|\n",
      "|16,8,304,150,3433...|\n",
      "|17,8,302,140,3449...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "raw_data2DF1 = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_data2DF1 = raw_data2.toDF()\n",
    "raw_data2DF1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.Dataset\n",
      "org.apache.spark.sql.Row[]\n",
      "org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n"
     ]
    }
   ],
   "source": [
    "println(getType(raw_data2DF1))\n",
    "println(getType(raw_data2DF1.take(1)))\n",
    "println(getType(raw_data2DF1.take(1)(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "//val raw_data2DF2 = raw_data2.map(x => Row(x)).toDF(\"k\")\n",
    "//val raw_data2DF3 = raw_data2.map(x => Row(x)).toDF()\n",
    "//raw_data2DF2.show(2)\n",
    "//raw_data2DF3.show(2)\n",
    "//error: value toDF is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_dataDF1 = [k: string]\n",
       "raw_dataDF2 = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_dataDF1 = raw_data1.toDF(\"k\")\n",
    "val raw_dataDF2 = raw_data1.toDF() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   k|\n",
      "+--------------------+\n",
      "|18,8,307,130,3504...|\n",
      "|15,8,350,165,3693...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|18,8,307,130,3504...|\n",
      "|15,8,350,165,3693...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dataDF1.show(2)\n",
    "raw_dataDF2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- k: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dataDF1.printSchema()\n",
    "raw_dataDF2.printSchema()\n",
    "raw_data2DF1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_dataDF = [value: string]\n",
       "d1 = [value: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: array<string>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raw_dataDF = raw_data1.toDF(\"value\")\n",
    "val d1  = raw_dataDF.selectExpr(\"split(value, ',') as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.Dataset\n",
      "org.apache.spark.sql.Row[]\n",
      "org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n"
     ]
    }
   ],
   "source": [
    "println(getType(d1))\n",
    "println(getType(d1.take(1)))\n",
    "println(getType(d1.take(1)(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[18, 8, 307, 130,...|\n",
      "|[15, 8, 350, 165,...|\n",
      "|[18, 8, 318, 150,...|\n",
      "|[16, 8, 304, 150,...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d2 = [value[0]: string, value[1]: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value[0]: string, value[1]: string ... 6 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d2 = d1.selectExpr(\"value[0]\", \"value[1]\", \"value[2]\", \"value[3]\", \"value[4]\", \"value[5]\", \"value[6]\", \"value[7]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|value[0]|value[1]|value[2]|value[3]|value[4]|value[5]|value[6]|value[7]|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|      18|       8|     307|     130|    3504|      12|      70|       1|\n",
      "|      15|       8|     350|     165|    3693|    11.5|      70|       1|\n",
      "|      18|       8|     318|     150|    3436|      11|      70|       1|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value[0]: string (nullable = true)\n",
      " |-- value[1]: string (nullable = true)\n",
      " |-- value[2]: string (nullable = true)\n",
      " |-- value[3]: string (nullable = true)\n",
      " |-- value[4]: string (nullable = true)\n",
      " |-- value[5]: string (nullable = true)\n",
      " |-- value[6]: string (nullable = true)\n",
      " |-- value[7]: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+----+---+---+\n",
      "|_c0|_c1|_c2|_c3| _c4| _c5|_c6|_c7|\n",
      "+---+---+---+---+----+----+---+---+\n",
      "| 18|  8|307|130|3504|  12| 70|  1|\n",
      "| 15|  8|350|165|3693|11.5| 70|  1|\n",
      "| 18|  8|318|150|3436|  11| 70|  1|\n",
      "+---+---+---+---+----+----+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(path).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
