{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4047)\" target=\"new_tab\">Spark UI: application_1540458187951_76727</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1540458187951_76727: Some(http://gw02.itversity.com:4047)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://rm01.itversity.com:19288/proxy/application_1540458187951_76727"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getType: (o: Any)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os_name = Linux\n",
       "hdfs_home = /user/kranthidr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /user/kranthidr/dataSets/spark-guide/retail-data/by-day/*.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/dataSets/spark-guide/retail-data/by-day/*.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = hdfs_home + \"/dataSets/spark-guide/retail-data/by-day/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "va = vecAssembler_cfe3ef22f905\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_cfe3ef22f905"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler()\n",
    "  .setInputCols(Array(\"Quantity\", \"UnitPrice\"))\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sales = va.transform(spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(path)\n",
    "  .limit(50)\n",
    "  .coalesce(1)\n",
    "  .where(\"Description IS NOT NULL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 7 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name (default: features)\n",
      "initMode: The initialization algorithm. Supported options: 'random' and 'k-means||'. (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 20)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: -1689246527)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "km = kmeans_18d0437c488b\n",
       "kmModel = kmeans_18d0437c488b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "kmeans_18d0437c488b"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "val km = new KMeans().setK(5)\n",
    "println(km.explainParams())\n",
    "val kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[4.933333333333334,4.326666666666667]\n",
      "[25.142857142857142,0.7842857142857143]\n",
      "[96.0,0.19]\n",
      "[12.916666666666666,1.5379166666666668]\n",
      "[60.0,0.675]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary = org.apache.spark.ml.clustering.KMeansSummary@6b619c58\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.clustering.KMeansSummary@6b619c58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val summary = kmModel.summary\n",
    "summary.clusterSizes // number of points\n",
    "kmModel.computeCost(sales)\n",
    "println(\"Cluster Centers: \")\n",
    "kmModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name (default: features)\n",
      "k: The desired number of leaf clusters. Must be > 1. (default: 4, current: 5)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 20, current: 5)\n",
      "minDivisibleClusterSize: The minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1.0)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: 566573821)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bkm = bisecting-kmeans_bcf67d633c54\n",
       "bkmModel = bisecting-kmeans_bcf67d633c54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bisecting-kmeans_bcf67d633c54"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.clustering.BisectingKMeans\n",
    "val bkm = new BisectingKMeans().setK(5).setMaxIter(5)\n",
    "println(bkm.explainParams())\n",
    "val bkmModel = bkm.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[4.933333333333334,4.326666666666667]\n",
      "[25.142857142857142,0.7842857142857143]\n",
      "[96.0,0.19]\n",
      "[12.916666666666666,1.5379166666666668]\n",
      "[60.0,0.675]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary = org.apache.spark.ml.clustering.BisectingKMeansSummary@3e4e9adf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.clustering.BisectingKMeansSummary@3e4e9adf"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val summary = bkmModel.summary\n",
    "summary.clusterSizes // number of points\n",
    "kmModel.computeCost(sales)\n",
    "println(\"Cluster Centers: \")\n",
    "kmModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name (default: features)\n",
      "k: Number of independent Gaussians in the mixture model. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "seed: random seed (default: 538009335)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 0.01)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gmm = GaussianMixture_dc0d9d0ea923\n",
       "model_gmm = GaussianMixture_dc0d9d0ea923\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GaussianMixture_dc0d9d0ea923"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.clustering.GaussianMixture\n",
    "val gmm = new GaussianMixture().setK(5)\n",
    "println(gmm.explainParams())\n",
    "val model_gmm = gmm.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary_gmm = org.apache.spark.ml.clustering.GaussianMixtureSummary@3c42c009\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.ml.clustering.GaussianMixtureSummary@3c42c009"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val summary_gmm = model_gmm.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1991321807468205, 0.3599996185757589, 0.10066410878117585, 0.2802046318346636, 0.059999460061581064]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gmm.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                mean|                 cov|\n",
      "+--------------------+--------------------+\n",
      "|[19.8151774282023...|22.83945042898985...|\n",
      "|[12.0000000000086...|4.96433050694389E...|\n",
      "|[69.2247322024240...|616.6686632794399...|\n",
      "|[5.15951718226872...|2.641134810285516...|\n",
      "|[12.6667556319647...|56.88841440247267...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         1|\n",
      "|         3|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[2.38840368747844...|\n",
      "|[1.14505451889683...|\n",
      "|[2.23075029088646...|\n",
      "|[0.01527721507281...|\n",
      "|[4.63362897307092...|\n",
      "|[4.63362897307092...|\n",
      "|[3.06607901274325...|\n",
      "|[0.01527721507281...|\n",
      "|[3.06607901274325...|\n",
      "|[2.74827980641886...|\n",
      "|[1.50512130345131...|\n",
      "|[5.45560429408012...|\n",
      "|[3.39654211927308...|\n",
      "|[2.54366071622309...|\n",
      "|[4.63362897307092...|\n",
      "|[3.06607901274325...|\n",
      "|[6.65561039981247...|\n",
      "|[1.60918876547942...|\n",
      "|[4.63362897307092...|\n",
      "|[0.99970112445006...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_gmm.gaussiansDF.show()\n",
    "summary_gmm.cluster.show()\n",
    "summary_gmm.clusterSizes\n",
    "summary_gmm.probability.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tkn = tok_ca4409a010ff\n",
       "tokenized = [InvoiceNo: string, StockCode: string ... 7 more fields]\n",
       "cv = cntVec_143848cd689a\n",
       "cvFitted = cntVec_143848cd689a\n",
       "prepped = [InvoiceNo: string, StockCode: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[InvoiceNo: string, StockCode: string ... 8 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.feature.{Tokenizer, CountVectorizer}\n",
    "val tkn = new Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "val tokenized = tkn.transform(sales.drop(\"features\"))\n",
    "val cv = new CountVectorizer()\n",
    "  .setInputCol(\"DescOut\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(500)\n",
    "  .setMinTF(0)\n",
    "  .setMinDF(0)\n",
    "  .setBinary(true)\n",
    "val cvFitted = cv.fit(tokenized)\n",
    "val prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\n",
      "docConcentration: Concentration parameter (commonly named \"alpha\") for the prior placed on documents' distributions over topics (\"theta\"). (undefined)\n",
      "featuresCol: features column name (default: features)\n",
      "k: The number of topics (clusters) to infer. Must be > 1. (default: 10, current: 10)\n",
      "keepLastCheckpoint: (For EM optimizer) If using checkpointing, this indicates whether to keep the last checkpoint. If false, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. (default: true)\n",
      "learningDecay: (For online optimizer) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. (default: 0.51)\n",
      "learningOffset: (For online optimizer) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. (default: 1024.0)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 20, current: 5)\n",
      "optimizeDocConcentration: (For online optimizer only, currently) Indicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. (default: true)\n",
      "optimizer: Optimizer or inference algorithm used to estimate the LDA model. Supported: online, em (default: online)\n",
      "seed: random seed (default: 1435876747)\n",
      "subsamplingRate: (For online optimizer) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. (default: 0.05)\n",
      "topicConcentration: Concentration parameter (commonly named \"beta\" or \"eta\") for the prior placed on topic' distributions over terms. (undefined)\n",
      "topicDistributionCol: Output column with estimates of the topic mixture distribution for each document (often called \"theta\" in the literature).  Returns a vector of zeros for an empty document. (default: topicDistribution)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lda = lda_7e821d969ebb\n",
       "model = lda_7e821d969ebb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lda_7e821d969ebb"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.ml.clustering.LDA\n",
    "val lda = new LDA().setK(10).setMaxIter(5)\n",
    "println(lda.explainParams())\n",
    "val model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+\n",
      "|topic|    termIndices|         termWeights|\n",
      "+-----+---------------+--------------------+\n",
      "|    0|   [69, 88, 81]|[0.00964392941038...|\n",
      "|    1|  [115, 5, 108]|[0.00939577470563...|\n",
      "|    2|   [20, 46, 44]|[0.01294729710614...|\n",
      "|    3|     [0, 6, 11]|[0.02158227144856...|\n",
      "|    4|    [66, 3, 54]|[0.00952138156731...|\n",
      "|    5|  [88, 85, 132]|[0.01277527418771...|\n",
      "|    6|     [2, 10, 0]|[0.01618353528599...|\n",
      "|    7|  [16, 15, 102]|[0.01597421231697...|\n",
      "|    8|   [49, 32, 48]|[0.01245161887934...|\n",
      "|    9|[101, 106, 119]|[0.01008978815501...|\n",
      "+-----+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[of, christmas, cases, set, vintage, mug, 6, heart, cake, mini, baking, box, white, wicker, red, metal, sign, feltcraft, spoon, decorative, marbles, 12, painted, crackers, design, portrait, small, milkmaid, blue, pack, silver, pudding, cup, bowl, 10, zinc, frame, and, loaf, tea, kit, pink, chain, paper, swirly, retrospot, bag, tree, egg, home, lights, sweet, calendar, this, cream, 72, bone, water, ingrid, 60, snowflakes, party, gliders, baubles, snack, princess, time, woolly, illustrated, engraved, star, hot, art, 4, pantry, designs, way, mirrored, regency, square, 50's, hairband, maids, ball, swallows, me, glass, blackboard, drink, hottie, biscuit, hen, reindeer, henrietta, fairy, 2, in, 3, open, 500g, chasing, milk, medium, closed, decoration, bubbles, skull, war, bottle, dog, 125g, decorations, lola, heart., asstd, hairbands, tins, cat, wooden, life, china, world, advent, chocolate, large, heidi, ice, antique, magnets, doll, tin, wall, billboard]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "model.describeTopics(3).show()\n",
    "cvFitted.vocabulary\n",
    "\n",
    "\n",
    "// COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
