{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4041)\" target=\"new_tab\">Spark UI: application_1540458187951_76578</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1540458187951_76578: Some(http://gw02.itversity.com:4041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://rm01.itversity.com:19288/proxy/application_1540458187951_76578"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getType: (o: Any)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os_name = Linux\n",
       "hdfs_home = /user/kranthidr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /user/kranthidr/dataSets/spark-guide/activity-data/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/dataSets/spark-guide/activity-data/"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = hdfs_home+\"/dataSets/spark-guide/activity-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "static = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val static = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streaming = [gt: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[gt: string, count: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val streaming = spark\n",
    "  .readStream\n",
    "  .schema(static.schema)\n",
    "  .option(\"maxFilesPerTrigger\", 10)\n",
    "  .json(path)\n",
    "  .groupBy(\"gt\")\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@170956c3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@170956c3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = streaming\n",
    "  .writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .option(\"checkpointLocation\", \"/tmp/\")\n",
    "  .queryName(\"test_stream\")\n",
    "  .format(\"memory\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"message\" : \"Getting offsets from FileStreamSource[hdfs://nn01.itversity.com:8020/user/kranthidr/dataSets/spark-guide/activity-data]\",\n",
       "  \"isDataAvailable\" : false,\n",
       "  \"isTriggerActive\" : true\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "  \"id\" : \"e69c04b1-694d-42fe-b858-3d3e0f253a00\",\n",
       "  \"runId\" : \"d5422c77-3e88-4fd3-8632-8d9308cc66f2\",\n",
       "  \"name\" : \"test_stream\",\n",
       "  \"timestamp\" : \"2019-03-30T09:35:09.946Z\",\n",
       "  \"batchId\" : 8,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"getOffset\" : 115,\n",
       "    \"triggerExecution\" : 295\n",
       "  },\n",
       "  \"stateOperators\" : [ ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"FileStreamSource[hdfs://nn01.itversity.com:8020/user/kranthidr/dataSets/spark-guide/activity-data]\",\n",
       "    \"startOffset\" : {\n",
       "      \"logOffset\" : 7\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"logOffset\" : 7\n",
       "    },\n",
       "    \"numInputRows\" : 0,\n",
       "    \"processedRowsPerSecond\" : 0.0\n",
       "  } ],\n",
       "  \"sink\" : {\n",
       "    \"description\" : \"MemorySink\"\n",
       "  }\n",
       "})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "  \"id\" : \"e69c04b1-694d-42fe-b858-3d3e0f253a00\",\n",
       "  \"runId\" : \"d5422c77-3e88-4fd3-8632-8d9308cc66f2\",\n",
       "  \"name\" : \"test_stream\",\n",
       "  \"timestamp\" : \"2019-03-30T09:35:09.946Z\",\n",
       "  \"batchId\" : 8,\n",
       "  \"numInputRows\" : 0,\n",
       "  \"processedRowsPerSecond\" : 0.0,\n",
       "  \"durationMs\" : {\n",
       "    \"getOffset\" : 115,\n",
       "    \"triggerExecution\" : 295\n",
       "  },\n",
       "  \"stateOperators\" : [ ],\n",
       "  \"sources\" : [ {\n",
       "    \"description\" : \"FileStreamSource[hdfs://nn01.itversity.com:8020/user/kranthidr/dataSets/spark-guide/activity-data]\",\n",
       "    \"startOffset\" : {\n",
       "      \"logOffset\" : 7\n",
       "    },\n",
       "    \"endOffset\" : {\n",
       "      \"logOffset\" : 7\n",
       "    },\n",
       "    \"numInputRows\" : 0,\n",
       "    \"processedRowsPerSecond\" : 0.0\n",
       "  } ],\n",
       "  \"sink\" : {\n",
       "    \"description\" : \"MemorySink\"\n",
       "  }\n",
       "}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "//val spark: SparkSession = ...\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.addListener(new StreamingQueryListener() {\n",
    "    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\n",
    "        println(\"Query started: \" + queryStarted.id)\n",
    "    }\n",
    "    override def onQueryTerminated(\n",
    "      queryTerminated: QueryTerminatedEvent): Unit = {\n",
    "        println(\"Query terminated: \" + queryTerminated.id)\n",
    "    }\n",
    "    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {\n",
    "        println(\"Query made progress: \" + queryProgress.progress)\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "\n",
    "// // import org.apache.kafka.clients.producer.KafkaProducer\n",
    "// // Message: <console>:16: error: object kafka is not a member of package org.apache\n",
    "// //        import org.apache.kafka.clients.producer.KafkaProducer\n",
    "\n",
    "// class KafkaMetrics(servers: String) extends StreamingQueryListener {\n",
    "    \n",
    "//   val kafkaProperties = new Properties()\n",
    "//   kafkaProperties.put(\n",
    "//     \"bootstrap.servers\",\n",
    "//     servers)\n",
    "//   kafkaProperties.put(\n",
    "//     \"key.serializer\",\n",
    "//     \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "//   kafkaProperties.put(\n",
    "//     \"value.serializer\",\n",
    "//     \"kafkashaded.org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "//   val producer = new KafkaProducer[String, String](kafkaProperties)\n",
    "\n",
    "//   override def onQueryProgress(event:\n",
    "//     StreamingQueryListener.QueryProgressEvent): Unit = {\n",
    "//     producer.send(new ProducerRecord(\"streaming-metrics\",\n",
    "//       event.progress.json))\n",
    "//   }\n",
    "//   override def onQueryStarted(event:\n",
    "//     StreamingQueryListener.QueryStartedEvent): Unit = {}\n",
    "//   override def onQueryTerminated(event:\n",
    "//     StreamingQueryListener.QueryTerminatedEvent): Unit = {}\n",
    "// }\n",
    "\n",
    "\n",
    "\n",
    "// // COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
