{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val csv_read = hdfs_home+\"/dataSets/spark-guide/flight-data/csv/2010-summary.csv\"\n",
    "val csv_write = hdfs_home + \"/temp/my-csv-file.tsv\"\n",
    "val tsv_write = hdfs_home + \"/temp/my-tsv-file.tsv\"\n",
    "\n",
    "val json_read = hdfs_home+\"/dataSets/spark-guide/flight-data/csv/2010-summary.json\"\n",
    "val json_write = hdfs_home + \"/temp/my-json-file.json\"\n",
    "\n",
    "val par_read = hdfs_home+\"/dataSets/spark-guide/flight-data/csv/2010-summary.parquet\"\n",
    "val par_write = hdfs_home + \"/temp/my-parquet-file.parquet\"\n",
    "\n",
    "val orc_read = hdfs_home+\"/dataSets/spark-guide/flight-data/csv/2010-summary.orc\"\n",
    "val orc_write = hdfs_home + \"/temp/my-orc-file.orc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val csv_df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(csv_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csv_df.write.format(\"csv\")\n",
    "  .option(\"mode\", \"OVERWRITE\")\n",
    "  .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "  .option(\"path\", csv_write)\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "\n",
    "val myManualSchema = new StructType(Array(\n",
    "  new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "  new StructField(\"count\", LongType, false)\n",
    "))\n",
    "\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .schema(myManualSchema)\n",
    "  .load(csv_read)\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val myManualSchema = new StructType(Array(\n",
    "                     new StructField(\"DEST_COUNTRY_NAME\", LongType, true),\n",
    "                     new StructField(\"ORIGIN_COUNTRY_NAME\", LongType, true),\n",
    "                     new StructField(\"count\", LongType, false) ))\n",
    "\n",
    "// spark.read.format(\"csv\")\n",
    "//   .option(\"header\", \"true\")\n",
    "//   .option(\"mode\", \"FAILFAST\")\n",
    "//   .schema(myManualSchema)\n",
    "//   .load(csv_read)\n",
    "//   .take(5)\n",
    "\n",
    "//org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val csvFile = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(csv_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\n",
    "  .save(tsv_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "//spark.read.format(\"json\")\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val json_df = spark.read.format(\"json\").option(\"mode\", \"FAILFAST\").schema(myManualSchema)\n",
    "  .load(json_read)\n",
    "json.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "json_df.write.format(\"json\").mode(\"overwrite\").save(json_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val par_df = spark.read.format(\"parquet\")\n",
    "  .load(par_read)\n",
    "par_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "par_df.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  .save(par_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val orc_df = spark.read.format(\"orc\").load(orc_read)\n",
    "orc_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "orc_df.write.format(\"orc\").mode(\"overwrite\").save(orc_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val driver =  \"org.sqlite.JDBC\"\n",
    "val path = \"/data/flight-data/jdbc/my-sqlite.db\"\n",
    "val url = s\"jdbc:sqlite:/${path}\"\n",
    "val tablename = \"flight_info\"\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "import java.sql.DriverManager\n",
    "val connection = DriverManager.getConnection(url)\n",
    "connection.isClosed()\n",
    "connection.close()\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\n",
    "  .option(\"dbtable\", tablename).option(\"driver\",  driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val pgDF = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\n",
    "  .option(\"url\", \"jdbc:postgresql://database_server\")\n",
    "  .option(\"dbtable\", \"schema.tablename\")\n",
    "  .option(\"user\", \"username\").option(\"password\",\"my-secret-password\").load()\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show(5)\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().explain\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')\").explain\n",
    "\n",
    "\n",
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)\n",
    "  AS flight_info\"\"\"\n",
    "\n",
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    "  .option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\",  driver)\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "dbDataFrame.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    "  .option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver)\n",
    "  .option(\"numPartitions\", 10).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val props = new java.util.Properties\n",
    "props.setProperty(\"driver\", \"org.sqlite.JDBC\")\n",
    "val predicates = Array(\n",
    "  \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.jdbc(url, tablename, predicates, props).show()\n",
    "spark.read.jdbc(url, tablename, predicates, props).rdd.getNumPartitions // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val props = new java.util.Properties\n",
    "props.setProperty(\"driver\", \"org.sqlite.JDBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predicates = Array(\n",
    "  \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\")\n",
    "spark.read.jdbc(url, tablename, predicates, props).count() // 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val colName = \"count\"\n",
    "val lowerBound = 0L\n",
    "val upperBound = 348113L // this is the max count in our database\n",
    "val numPartitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "spark.read.jdbc(url,tablename,colName,lowerBound,upperBound,numPartitions,props)\n",
    "  .count() // 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
    "csvFile.write.mode(\"overwrite\").jdbc(newPath, tablename, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "spark.read.jdbc(newPath, tablename, props).count() // 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csvFile.write.mode(\"append\").jdbc(newPath, tablename, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "spark.read.jdbc(newPath, tablename, props).count() // 765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "spark.read.textFile(\"/data/flight-data/csv/2010-summary.csv\")\n",
    "  .selectExpr(\"split(value, ',') as rows\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\")\n",
    "  .write.partitionBy(\"count\").text(\"/tmp/five-csv-files2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    "  .save(\"/tmp/partitioned-files.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    "  .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")\n",
    "\n",
    "\n",
    "// COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
