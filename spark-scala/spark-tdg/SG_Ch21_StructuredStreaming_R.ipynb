{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4041)\" target=\"new_tab\">Spark UI: application_1540458187951_76576</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1540458187951_76576: Some(http://gw02.itversity.com:4041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://rm01.itversity.com:19288/proxy/application_1540458187951_76576"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getType: (o: Any)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os_name = Linux\n",
       "hdfs_home = /user/kranthidr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /user/kranthidr/dataSets/spark-guide/activity-data/\n",
       "path1 = /user/kranthidr/dataSets/spark-guide/flight-data/parquet/2010-summary.parquet/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/dataSets/spark-guide/flight-data/parquet/2010-summary.parquet/"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = hdfs_home+\"/dataSets/spark-guide/activity-data/\"\n",
    "val path1 = hdfs_home+\"/dataSets/spark-guide/flight-data/parquet/2010-summary.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "static = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n",
       "dataSchema = StructType(StructField(Arrival_Time,LongType,true), StructField(Creation_Time,LongType,true), StructField(Device,StringType,true), StructField(Index,LongType,true), StructField(Model,StringType,true), StructField(User,StringType,true), StructField(gt,StringType,true), StructField(x,DoubleType,true), StructField(y,DoubleType,true), StructField(z,DoubleType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(Arrival_Time,LongType,true), StructField(Creation_Time,LongType,true), StructField(Device,StringType,true), StructField(Index,LongType,true), StructField(Model,StringType,true), StructField(User,StringType,true), StructField(gt,StringType,true), StructField(x,DoubleType,true), StructField(y,DoubleType,true), StructField(z,DoubleType,true))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val static = spark.read.json(path)\n",
    "val dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streaming = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val streaming = spark.readStream.schema(dataSchema)\n",
    "  .option(\"maxFilesPerTrigger\", 1).json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activityCounts = [gt: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[gt: string, count: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activityQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4e0b4a69\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4e0b4a69"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\n",
    "  .format(\"memory\").outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "//activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4e0b4a69]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|12309|\n",
      "|     stand|11384|\n",
      "|stairsdown| 9365|\n",
      "|      walk|13256|\n",
      "|  stairsup|10452|\n",
      "|      null|10449|\n",
      "|      bike|10796|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|12309|\n",
      "|     stand|11384|\n",
      "|stairsdown| 9365|\n",
      "|      walk|13256|\n",
      "|  stairsup|10452|\n",
      "|      null|10449|\n",
      "|      bike|10796|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|12309|\n",
      "|     stand|11384|\n",
      "|stairsdown| 9365|\n",
      "|      walk|13256|\n",
      "|  stairsup|10452|\n",
      "|      null|10449|\n",
      "|      bike|10796|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|24619|\n",
      "|     stand|22769|\n",
      "|stairsdown|18729|\n",
      "|      walk|26512|\n",
      "|  stairsup|20905|\n",
      "|      null|20896|\n",
      "|      bike|21593|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "for( i <- 1 to 5 ) {\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    Thread.sleep(1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpleTransform = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6e6ee1ea\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6e6ee1ea"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.expr\n",
    "val simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\n",
    "  .where(\"stairs\")\n",
    "  .where(\"gt is not null\")\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\n",
    "  .writeStream\n",
    "  .queryName(\"simple_transform\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"append\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deviceModelStats = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1e74cac7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1e74cac7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\n",
    "  .drop(\"avg(Arrival_time)\")\n",
    "  .drop(\"avg(Creation_Time)\")\n",
    "  .drop(\"avg(Index)\")\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\").outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "// val historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "\n",
    "// val deviceModelStats_aggr = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\n",
    "//   .cube(\"gt\", \"model\").avg()\n",
    "//   .join(historicalAgg, Seq(\"gt\", \"model\"))\n",
    "//   .writeStream.queryName(\"device_counts_aggr\").format(\"memory\").outputMode(\"complete\")\n",
    "//   .start()\n",
    "\n",
    "//Taking too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "// Subscribe to 1 topic\n",
    "// val ds1 = spark.readStream.format(\"kafka\")\n",
    "//   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "//   .option(\"subscribe\", \"topic1\")\n",
    "//   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Subscribe to multiple topics\n",
    "// val ds2 = spark.readStream.format(\"kafka\")\n",
    "//   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "//   .option(\"subscribe\", \"topic1,topic2\")\n",
    "//   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Subscribe to a pattern of topics\n",
    "// val ds3 = spark.readStream.format(\"kafka\")\n",
    "//   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "//   .option(\"subscribePattern\", \"topic.*\")\n",
    "//   .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "// ds1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "//   .writeStream.format(\"kafka\")\n",
    "//   .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\n",
    "//   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "//   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ds1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "//   .writeStream.format(\"kafka\")\n",
    "//   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "//   .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "//   .option(\"topic\", \"topic1\")\n",
    "//   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "//in Scala\n",
    "// datasetOfString.write.foreach(new ForeachWriter[String] {\n",
    "//   def open(partitionId: Long, version: Long): Boolean = {\n",
    "//     // open a database connection\n",
    "//   }\n",
    "//   def process(record: String) = {\n",
    "//     // write string to connection\n",
    "//   }\n",
    "//   def close(errorOrNull: Throwable): Unit = {\n",
    "//     // close the connection\n",
    "//   }\n",
    "// })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "// val socketDF = spark.readStream.format(\"socket\")\n",
    "//   .option(\"host\", \"localhost\").option(\"port\", 9999).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "//activityCounts.format(\"console\").write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.streaming.DataStreamWriter@74112522"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "activityCounts.writeStream.format(\"memory\").queryName(\"my_device_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@e82711d"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|12309|\n",
      "|     stand|11384|\n",
      "|stairsdown| 9365|\n",
      "|      walk|13256|\n",
      "|  stairsup|10452|\n",
      "|      null|10449|\n",
      "|      bike|10796|\n",
      "+----------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|24619|\n",
      "|     stand|22769|\n",
      "|stairsdown|18729|\n",
      "|      walk|26512|\n",
      "|  stairsup|20905|\n",
      "|      null|20896|\n",
      "|      bike|21593|\n",
      "+----------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|stairsdown|28094|\n",
      "|      walk|39768|\n",
      "|  stairsup|31357|\n",
      "|      null|31343|\n",
      "|      bike|32390|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "activityCounts.writeStream.trigger(Trigger.ProcessingTime(\"100 seconds\"))\n",
    "  .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@28a6f65b"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|       sit|12309|\n",
      "|     stand|11384|\n",
      "|stairsdown| 9365|\n",
      "|      walk|13256|\n",
      "|  stairsup|10452|\n",
      "|      null|10449|\n",
      "|      bike|10796|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "activityCounts.writeStream.trigger(Trigger.Once())\n",
    "  .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Flight\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String,\n",
    "  count: BigInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSchema = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataSchema = spark.read.parquet(path1).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsDF = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsDF = spark.readStream.schema(dataSchema).parquet(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = flightsDF.as[Flight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originIsDestination: (flight_row: Flight)Boolean\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def originIsDestination(flight_row: Flight): Boolean = {\n",
    "  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5fbe457"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.filter(flight_row => originIsDestination(flight_row))\n",
    "  .groupByKey(x => x.DEST_COUNTRY_NAME).count()\n",
    "  .writeStream.queryName(\"device_counts_ds\").format(\"memory\").outputMode(\"complete\")\n",
    "  .start()\n",
    "\n",
    "\n",
    "// COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|        value|count(1)|\n",
      "+-------------+--------+\n",
      "|United States|       1|\n",
      "+-------------+--------+\n",
      "\n",
      "+-------------+--------+\n",
      "|        value|count(1)|\n",
      "+-------------+--------+\n",
      "|United States|       1|\n",
      "+-------------+--------+\n",
      "\n",
      "+-------------+--------+\n",
      "|        value|count(1)|\n",
      "+-------------+--------+\n",
      "|United States|       1|\n",
      "+-------------+--------+\n",
      "\n",
      "+-------------+--------+\n",
      "|        value|count(1)|\n",
      "+-------------+--------+\n",
      "|United States|       1|\n",
      "+-------------+--------+\n",
      "\n",
      "+-------------+--------+\n",
      "|        value|count(1)|\n",
      "+-------------+--------+\n",
      "|United States|       1|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for( i <- 1 to 5 ) {\n",
    "    spark.sql(\"SELECT * FROM device_counts_ds\").show()\n",
    "    Thread.sleep(1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
