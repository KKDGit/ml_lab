{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4044)\" target=\"new_tab\">Spark UI: application_1540458187951_76577</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1540458187951_76577: Some(http://gw02.itversity.com:4044)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://rm01.itversity.com:19288/proxy/application_1540458187951_76577"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll.filter(_._2.contains(\"/proxy/\"))(0)._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getType: (o: Any)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getType(o: Any) = o.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os_name = Linux\n",
       "hdfs_home = /user/kranthidr\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val os_name = System.getProperty(\"os.name\")\n",
    "val hdfs_home = \"/user/\" + System.getenv(\"HOME\").split(\"/\")(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = /user/kranthidr/dataSets/spark-guide/activity-data/\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/user/kranthidr/dataSets/spark-guide/activity-data/"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = hdfs_home+\"/dataSets/spark-guide/activity-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// in Scala\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "static = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val static = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streaming = [Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 8 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val streaming = spark\n",
    "  .readStream\n",
    "  .schema(static.schema)\n",
    "  .option(\"maxFilesPerTrigger\", 10)\n",
    "  .json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withEventTime = [Arrival_Time: bigint, Creation_Time: bigint ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Arrival_Time: bigint, Creation_Time: bigint ... 9 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "val withEventTime = streaming.selectExpr(\n",
    "  \"*\",\n",
    "  \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4f2ce925"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{window, col}\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\n",
    "  .writeStream\n",
    "  .queryName(\"events_per_window\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "spark.sql(\"SELECT * FROM events_per_window\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@556abaac"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{window, col}\n",
    "\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), col(\"User\")).count()\n",
    "  .writeStream\n",
    "  .queryName(\"events_per_window1\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5a1210a"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{window, col}\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\n",
    "  .count()\n",
    "  .writeStream\n",
    "  .queryName(\"events_per_window2\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@44adbf0d"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.{window, col}\n",
    "withEventTime\n",
    "  .withWatermark(\"event_time\", \"5 hours\")\n",
    "  .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\n",
    "  .count()\n",
    "  .writeStream\n",
    "  .queryName(\"events_per_window3\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77f291bf"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "withEventTime\n",
    "  .withWatermark(\"event_time\", \"5 seconds\")\n",
    "  .dropDuplicates(\"User\", \"event_time\")\n",
    "  .groupBy(\"User\")\n",
    "  .count()\n",
    "  .writeStream\n",
    "  .queryName(\"deduplicated\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "case class InputRow(user:String, timestamp:java.sql.Timestamp, activity:String)\n",
    "case class UserState(user:String,\n",
    "  var activity:String,\n",
    "  var start:java.sql.Timestamp,\n",
    "  var end:java.sql.Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateUserStateWithEvent: (state: UserState, input: InputRow)UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "def updateUserStateWithEvent(state:UserState, input:InputRow):UserState = {\n",
    "  if (Option(input.timestamp).isEmpty) {\n",
    "    return state\n",
    "  }\n",
    "  if (state.activity == input.activity) {\n",
    "\n",
    "    if (input.timestamp.after(state.end)) {\n",
    "      state.end = input.timestamp\n",
    "    }\n",
    "    if (input.timestamp.before(state.start)) {\n",
    "      state.start = input.timestamp\n",
    "    }\n",
    "  } else {\n",
    "    if (input.timestamp.after(state.end)) {\n",
    "      state.start = input.timestamp\n",
    "      state.end = input.timestamp\n",
    "      state.activity = input.activity\n",
    "    }\n",
    "  }\n",
    "\n",
    "  state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (user: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[UserState])UserState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}\n",
    "def updateAcrossEvents(user:String,\n",
    "  inputs: Iterator[InputRow],\n",
    "  oldState: GroupState[UserState]):UserState = {\n",
    "  var state:UserState = if (oldState.exists) oldState.get else UserState(user,\n",
    "        \"\",\n",
    "        new java.sql.Timestamp(6284160000000L),\n",
    "        new java.sql.Timestamp(6284160L)\n",
    "    )\n",
    "  // we simply specify an old date that we can compare against and\n",
    "  // immediately update based on the values in our data\n",
    "\n",
    "  for (input <- inputs) {\n",
    "    state = updateUserStateWithEvent(state, input)\n",
    "    oldState.update(state)\n",
    "  }\n",
    "  state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2604e04"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "withEventTime\n",
    "  .selectExpr(\"User as user\",\n",
    "    \"cast(Creation_Time/1000000000 as timestamp) as timestamp\", \"gt as activity\")\n",
    "  .as[InputRow]\n",
    "  .groupByKey(_.user)\n",
    "  .mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAcrossEvents)\n",
    "  .writeStream\n",
    "  .queryName(\"mapGroupsWithState\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"update\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class DeviceState\n",
       "defined class OutputRow\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "case class InputRow(device: String, timestamp: java.sql.Timestamp, x: Double)\n",
    "case class DeviceState(device: String, var values: Array[Double],\n",
    "  var count: Int)\n",
    "case class OutputRow(device: String, previousAverage: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateWithEvent: (state: DeviceState, input: InputRow)DeviceState\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "def updateWithEvent(state:DeviceState, input:InputRow):DeviceState = {\n",
    "  state.count += 1\n",
    "  // maintain an array of the x-axis values\n",
    "  state.values = state.values ++ Array(input.x)\n",
    "  state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode,\n",
    "  GroupState}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (device: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[DeviceState])Iterator[OutputRow]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updateAcrossEvents(device:String, inputs: Iterator[InputRow],\n",
    "  oldState: GroupState[DeviceState]):Iterator[OutputRow] = {\n",
    "  inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>\n",
    "    val state = if (oldState.exists) oldState.get\n",
    "      else DeviceState(device, Array(), 0)\n",
    "\n",
    "    val newState = updateWithEvent(state, input)\n",
    "    if (newState.count >= 500) {\n",
    "      // One of our windows is complete; replace our state with an empty\n",
    "      // DeviceState and output the average for the past 500 items from\n",
    "      // the old state\n",
    "      oldState.update(DeviceState(device, Array(), 0))\n",
    "      Iterator(OutputRow(device,\n",
    "        newState.values.sum / newState.values.length.toDouble))\n",
    "    }\n",
    "    else {\n",
    "      // Update the current DeviceState object in place and output no\n",
    "      // records\n",
    "      oldState.update(newState)\n",
    "      Iterator()\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1e1d7b58"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "\n",
    "withEventTime\n",
    "  .selectExpr(\"Device as device\",\n",
    "    \"cast(Creation_Time/1000000000 as timestamp) as timestamp\", \"x\")\n",
    "  .as[InputRow]\n",
    "  .groupByKey(_.device)\n",
    "  .flatMapGroupsWithState(OutputMode.Append,\n",
    "    GroupStateTimeout.NoTimeout)(updateAcrossEvents)\n",
    "  .writeStream\n",
    "  .queryName(\"flatMapGroupsWithState\")\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"append\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class InputRow\n",
       "defined class UserSession\n",
       "defined class UserSessionOutput\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "case class InputRow(uid:String, timestamp:java.sql.Timestamp, x:Double,\n",
    "  activity:String)\n",
    "case class UserSession(val uid:String, var timestamp:java.sql.Timestamp,\n",
    "  var activities: Array[String], var values: Array[Double])\n",
    "case class UserSessionOutput(val uid:String, var activities: Array[String],\n",
    "  var xAvg:Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateWithEvent: (state: UserSession, input: InputRow)UserSession\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "def updateWithEvent(state:UserSession, input:InputRow):UserSession = {\n",
    "  // handle malformed dates\n",
    "  if (Option(input.timestamp).isEmpty) {\n",
    "    return state\n",
    "  }\n",
    "\n",
    "  state.timestamp = input.timestamp\n",
    "  state.values = state.values ++ Array(input.x)\n",
    "  if (!state.activities.contains(input.activity)) {\n",
    "    state.activities = state.activities ++ Array(input.activity)\n",
    "  }\n",
    "  state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateAcrossEvents: (uid: String, inputs: Iterator[InputRow], oldState: org.apache.spark.sql.streaming.GroupState[UserSession])Iterator[UserSessionOutput]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode,\n",
    "  GroupState}\n",
    "\n",
    "def updateAcrossEvents(uid:String,\n",
    "  inputs: Iterator[InputRow],\n",
    "  oldState: GroupState[UserSession]):Iterator[UserSessionOutput] = {\n",
    "\n",
    "  inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>\n",
    "    val state = if (oldState.exists) oldState.get else UserSession(\n",
    "    uid,\n",
    "    new java.sql.Timestamp(6284160000000L),\n",
    "    Array(),\n",
    "    Array())\n",
    "    val newState = updateWithEvent(state, input)\n",
    "\n",
    "    if (oldState.hasTimedOut) {\n",
    "      val state = oldState.get\n",
    "      oldState.remove()\n",
    "      Iterator(UserSessionOutput(uid,\n",
    "      state.activities,\n",
    "      newState.values.sum / newState.values.length.toDouble))\n",
    "    } else if (state.values.length > 1000) {\n",
    "      val state = oldState.get\n",
    "      oldState.remove()\n",
    "      Iterator(UserSessionOutput(uid,\n",
    "      state.activities,\n",
    "      newState.values.sum / newState.values.length.toDouble))\n",
    "    } else {\n",
    "      oldState.update(newState)\n",
    "      oldState.setTimeoutTimestamp(newState.timestamp.getTime(), \"5 seconds\")\n",
    "      Iterator()\n",
    "    }\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@23eb1fe3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "import org.apache.spark.sql.streaming.GroupStateTimeout\n",
    "\n",
    "withEventTime.where(\"x is not null\")\n",
    "  .selectExpr(\"user as uid\",\n",
    "    \"cast(Creation_Time/1000000000 as timestamp) as timestamp\",\n",
    "    \"x\", \"gt as activity\")\n",
    "  .as[InputRow]\n",
    "  .withWatermark(\"timestamp\", \"5 seconds\")\n",
    "  .groupByKey(_.uid)\n",
    "  .flatMapGroupsWithState(OutputMode.Append,\n",
    "    GroupStateTimeout.EventTimeTimeout)(updateAcrossEvents)\n",
    "  .writeStream\n",
    "  .queryName(\"count_based_device\")\n",
    "  .format(\"memory\")\n",
    "  .start()\n",
    "\n",
    "\n",
    "// COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@556abaac, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@23eb1fe3, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5a1210a, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@44adbf0d, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2604e04, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1e1d7b58, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77f291bf, org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4f2ce925]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
