{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.io.{BufferedWriter, FileWriter}\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "import au.com.bytecode.opencsv.CSVWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_kScore1 = Vector((20,6.190500406143229E7), (40,2.1488629588922817E7), (60,1.6335789061936911E7), (80,4897884.209190392), (100,7703198.294787648), (120,7579853.472105312), (140,8292717.062987187), (160,6412945.578045533), (180,2588438.9831387093), (200,7358031.344011094), (220,2016958.1361289758), (240,4624509.75620643), (260,2369998.326037962))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector((20,6.190500406143229E7), (40,2.1488629588922817E7), (60,1.6335789061936911E7), (80,4897884.209190392), (100,7703198.294787648), (120,7579853.472105312), (140,8292717.062987187), (160,6412945.578045533), (180,2588438.9831387093), (200,7358031.344011094), (220,2016958.1361289758), (240,4624509.75620643), (260,2369998.326037962))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val k_kScore1 = Vector((20,6.190500406143229E7), (40,2.1488629588922817E7), (60,1.6335789061936911E7), (80,4897884.209190392), (100,7703198.294787648), (120,7579853.472105312), (140,8292717.062987187), (160,6412945.578045533), (180,2588438.9831387093), (200,7358031.344011094), (220,2016958.1361289758), (240,4624509.75620643), (260,2369998.326037962))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://gw02.itversity.com:4048)\" target=\"new_tab\">Spark UI: application_1533622723243_5192</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark application_1533622723243_5192: Some(http://gw02.itversity.com:4048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.11.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.3.0.2.6.5.0-292"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.history.kerberos.keytab,none)\n",
      "(spark.driver.host,gw02.itversity.com)\n",
      "(spark.history.fs.logDirectory,hdfs:///spark2-history/)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.driver.port,53652)\n",
      "(spark.shuffle.service.enabled,true)\n",
      "(spark.driver.extraLibraryPath,/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64)\n",
      "(spark.yarn.queue,default)\n",
      "(spark.repl.class.uri,spark://gw02.itversity.com:53652/classes)\n",
      "(spark.jars,file:/home/kranthidr/.local/share/jupyter/kernels/spark_scala/lib/toree-assembly-0.2.0-incubating.jar)\n",
      "(spark.yarn.historyServer.address,gw03.itversity.com:18081)\n",
      "(spark.repl.class.outputDir,/tmp/spark-c7360dec-4a39-4746-99dc-577ea2e9b699/repl-531d44a2-260e-480e-8985-590f6a6516c5)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.history.kerberos.principal,none)\n",
      "(spark.dynamicAllocation.maxExecutors,4)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,yarn)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.history.provider,org.apache.spark.deploy.history.FsHistoryProvider)\n",
      "(spark.executor.extraLibraryPath,/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64)\n",
      "(spark.eventLog.dir,hdfs:///spark2-history/)\n",
      "(spark.dynamicAllocation.enabled,true)\n",
      "(spark.history.ui.port,18081)\n",
      "(spark.driver.appUIAddress,http://gw02.itversity.com:4048)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,rm01.itversity.com)\n",
      "(spark.dynamicAllocation.minExecutors,0)\n",
      "(spark.dynamicAllocation.initialExecutors,0)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://rm01.itversity.com:19288/proxy/application_1533622723243_5192)\n",
      "(spark.dynamicAllocation.executorIdleTimeout,10)\n",
      "(spark.app.id,application_1533622723243_5192)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputFile = java.io.BufferedWriter@64f981e2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "java.io.BufferedWriter@64f981e2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputFile = new BufferedWriter(new FileWriter(\"./output.csv\"))\n",
    "//this will create an output file which is an output.csv file in the said directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvWriter = au.com.bytecode.opencsv.CSVWriter@54f63894\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "au.com.bytecode.opencsv.CSVWriter@54f63894"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvWriter = new CSVWriter(outputFile) \n",
    "// this will create a csvwriter object which will have the outputFile in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvSchema = Array(n_clusters, cost)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[n_clusters, cost]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvSchema = Array(\"n_clusters\", \"cost\") \n",
    "// this is the schema for your CSV file, in my case I have four fields, \n",
    "//you can include the schema if you want itâ€™s totally optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_clusters_list = List(20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260)\n",
       "cost_list = List(6.190500406143229E7, 2.1488629588922817E7, 1.6335789061936911E7, 4897884.209190392, 7703198.294787648, 7579853.472105312, 8292717.062987187, 6412945.578045533, 2588438.9831387093, 7358031.344011094, 2016958.1361289758, 4624509.75620643, 2369998.326037962)\n",
       "listOfRecords = ListBuffer(Array(n_clusters, cost), Array(20, 6.190500406143229E7), Array(40, 2.1488629588922817E7), Array(60, 1.6335789061936911E7), Array(80, 4897884.209190392), Array(100, 7703198.294787648), Array(120, 7579853.472105312), Array(140, 8292717.062987187), Array(160, 6412945.578045533), Array(180, 2588438.9831387093), Array(200, 7358031.34401109...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer([Ljava.lang.String;@16bfe44d, [Ljava.lang.String;@2c555382, [Ljava.lang.String;@230976ad, [Ljava.lang.String;@5d62f1d1, [Ljava.lang.String;@6ed168bf, [Ljava.lang.String;@689effa4, [Ljava.lang.String;@ca5004, [Ljava.lang.String;@4a6cd5b5, [Ljava.lang.String;@2f9dc8d7, [Ljava.lang.String;@3061615f, [Ljava.lang.String;@62f95d27, [Ljava.lang.String;@4e94fdb0, [Ljava.lang.String;@f0e5fc5, [Ljava.lang.String;@345eace2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n_clusters_list = k_kScore1.map(_._1).toList\n",
    "val cost_list = k_kScore1.map(_._2).toList\n",
    "\n",
    "var listOfRecords = new ListBuffer[Array[String]]() \n",
    "// here is the list buffer which holds all the records\n",
    "listOfRecords += csvSchema //this is how we add the fields to our CSV file.\n",
    "\n",
    "//listOfRecords += Array(n_clusters_list.toString, cost_list.toString)\n",
    "for (i <- 0 to (n_clusters_list.length-1)) {\n",
    "    listOfRecords += Array(n_clusters_list(i).toString, cost_list(i).toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvWriter.writeAll(listOfRecords.toList) \n",
    "//here we are writing all the records to the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile .close() \n",
    "// here we will finally close the file after writing all the records into it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
