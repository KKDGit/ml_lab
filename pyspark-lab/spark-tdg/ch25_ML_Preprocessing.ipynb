{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/hdp/current/spark2-client'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    ".config(\"spark.driver.memory\", \"8g\")\\\n",
    ".config(\"spark.executor.memory\", \"3g\")\\\n",
    ".appName(\"ch25_MLPreprocessing\").getOrCreate()\n",
    "\n",
    "# .config(\"spark.executor.cores\", \"5\")\\\n",
    "# .config(\"spark.executor.instances\", \"10\")\\\n",
    "\n",
    "\n",
    "# This file is sourced when running various Spark programs.\n",
    "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
    "\n",
    "# Options read in YARN client mode\n",
    "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n",
    "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n",
    "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n",
    "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://rm01.itversity.com:19288/proxy/application_1533622723243_15480\n"
     ]
    }
   ],
   "source": [
    "for x in sc._conf.getAll():\n",
    "    if \"/proxy/\" in x[1]:\n",
    "        print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/user/kranthidr/dataSets/spark-guide/retail-data/by-day/*.csv\")\\\n",
    "  .coalesce(5)\\\n",
    "  .where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeIntDF = spark.read.parquet(\"/user/kranthidr/dataSets/spark-guide/simple-ml-integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simpleDF = spark.read.json(\"/user/kranthidr/dataSets/spark-guide/simple-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaleDF = spark.read.parquet(\"/user/kranthidr/dataSets/spark-guide/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                                                              |label|\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n",
      "|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n",
      "|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervised.fit(simpleDF).transform(simpleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+---------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                         |label|\n",
      "+-----+----+------+------------------+---------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|[0.0,1.0,1.0,14.386294994851129] |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|[0.0,0.0,8.0,14.386294994851129] |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|[0.0,0.0,12.0,14.386294994851129]|0.0  |\n",
      "|green|good|15    |38.97187133755819 |[0.0,1.0,15.0,38.97187133755819] |1.0  |\n",
      "|green|good|12    |14.386294994851129|[0.0,1.0,12.0,14.386294994851129]|1.0  |\n",
      "+-----+----+------+------------------+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supervised1 = RFormula(formula=\"lab ~ .\")\n",
    "supervised1.fit(simpleDF).transform(simpleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "  .setStatement(\"\"\"\n",
    "    SELECT sum(Quantity), count(*), CustomerID\n",
    "    FROM __THIS__\n",
    "    GROUP BY CustomerID\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          290|      98|   13607.0|\n",
      "|         1542|      30|   13094.0|\n",
      "|          541|      27|   14285.0|\n",
      "|           34|       6|   14768.0|\n",
      "|           97|      12|   16596.0|\n",
      "|          630|      72|   17633.0|\n",
      "|          244|      31|   16561.0|\n",
      "|          493|      64|   16629.0|\n",
      "|          159|      38|   17267.0|\n",
      "|          206|      23|   12493.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basicTransformation.transform(sales).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_4e3c8994744c6a3cd411__output|\n",
      "+----+----+----+--------------------------------------------+\n",
      "|   1|   2|   3|                               [1.0,2.0,3.0]|\n",
      "|   4|   5|   6|                               [4.0,5.0,6.0]|\n",
      "|   7|   8|   9|                               [7.0,8.0,9.0]|\n",
      "+----+----+----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------+\n",
      "|  id|Bucketizer_43b99bb609f5a474846f__output|\n",
      "+----+---------------------------------------+\n",
      "| 0.0|                                    0.0|\n",
      "| 1.0|                                    0.0|\n",
      "| 2.0|                                    0.0|\n",
      "| 3.0|                                    0.0|\n",
      "| 4.0|                                    0.0|\n",
      "| 5.0|                                    1.0|\n",
      "| 6.0|                                    1.0|\n",
      "| 7.0|                                    1.0|\n",
      "| 8.0|                                    1.0|\n",
      "| 9.0|                                    1.0|\n",
      "|10.0|                                    2.0|\n",
      "|11.0|                                    2.0|\n",
      "|12.0|                                    2.0|\n",
      "|13.0|                                    2.0|\n",
      "|14.0|                                    2.0|\n",
      "|15.0|                                    2.0|\n",
      "|16.0|                                    2.0|\n",
      "|17.0|                                    2.0|\n",
      "|18.0|                                    2.0|\n",
      "|19.0|                                    2.0|\n",
      "+----+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------------+\n",
      "|  id|QuantileDiscretizer_401dafe89f7b762b31c4__output|\n",
      "+----+------------------------------------------------+\n",
      "| 0.0|                                             0.0|\n",
      "| 1.0|                                             0.0|\n",
      "| 2.0|                                             0.0|\n",
      "| 3.0|                                             1.0|\n",
      "| 4.0|                                             1.0|\n",
      "| 5.0|                                             1.0|\n",
      "| 6.0|                                             1.0|\n",
      "| 7.0|                                             2.0|\n",
      "| 8.0|                                             2.0|\n",
      "| 9.0|                                             2.0|\n",
      "|10.0|                                             2.0|\n",
      "|11.0|                                             2.0|\n",
      "|12.0|                                             3.0|\n",
      "|13.0|                                             3.0|\n",
      "|14.0|                                             3.0|\n",
      "|15.0|                                             4.0|\n",
      "|16.0|                                             4.0|\n",
      "|17.0|                                             4.0|\n",
      "|18.0|                                             4.0|\n",
      "|19.0|                                             4.0|\n",
      "+----+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |StandardScaler_4e2e9d6499c1f481920f__output                 |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|0  |[1.0,0.1,-1.0]|[1.1952286093343936,0.02337622911060922,-0.5976143046671968]|\n",
      "|1  |[2.0,1.1,1.0] |[2.390457218668787,0.2571385202167014,0.5976143046671968]   |\n",
      "|1  |[3.0,10.1,3.0]|[3.5856858280031805,2.3609991401715313,1.7928429140015902]  |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "|id |features      |MinMaxScaler_424a83ef26ce22fbe448__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[5.0,5.0,5.0]                            |\n",
      "|1  |[2.0,1.1,1.0] |[7.5,5.5,7.5]                            |\n",
      "|0  |[1.0,0.1,-1.0]|[5.0,5.0,5.0]                            |\n",
      "|1  |[2.0,1.1,1.0] |[7.5,5.5,7.5]                            |\n",
      "|1  |[3.0,10.1,3.0]|[10.0,10.0,10.0]                         |\n",
      "+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------------------------+\n",
      "|id |features      |MaxAbsScaler_45d88e4a8c0993c983b8__output                    |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.3333333333333333,0.009900990099009901,-0.3333333333333333]|\n",
      "|1  |[2.0,1.1,1.0] |[0.6666666666666666,0.10891089108910892,0.3333333333333333]  |\n",
      "|1  |[3.0,10.1,3.0]|[1.0,1.0,1.0]                                                |\n",
      "+---+--------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "  .setScalingVec(scaleUpVec)\\\n",
    "  .setInputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------+\n",
      "| id|      features|ElementwiseProduct_4f0ca9f6be0eb8db3677__output|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                              [30.0,151.5,60.0]|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------------------------------+\n",
      "|id |features      |Normalizer_4f569092b31f528b583d__output                        |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.47619047619047616,0.047619047619047616,-0.47619047619047616]|\n",
      "|1  |[2.0,1.1,1.0] |[0.48780487804878053,0.26829268292682934,0.24390243902439027]  |\n",
      "|1  |[3.0,10.1,3.0]|[0.18633540372670807,0.6273291925465838,0.18633540372670807]   |\n",
      "+---+--------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_44b9aa81a342b03af7c2__output|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                                      good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                                       bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                                       bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                                      good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                                      good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                                       bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                                       bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|     1|14.386294994851129|     1.0|                                      good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                                       bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                                       bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                                      good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                                      good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                                       bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                                      good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                                       bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                                       bad|\n",
      "+-----+----+------+------------------+--------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxIn = spark.createDataFrame([\n",
    "  (Vectors.dense(1, 2, 3),1),\n",
    "  (Vectors.dense(2, 5, 6),2),\n",
    "  (Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxIn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(DenseVector([1.0, 2.0, 3.0]), 1),\n",
       " (DenseVector([2.0, 5.0, 6.0]), 2),\n",
       " (DenseVector([1.0, 8.0, 9.0]), 3)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  (Vectors.dense(1, 2, 3),1),\n",
    "  (Vectors.dense(2, 5, 6),2),\n",
    "  (Vectors.dense(1, 8, 9),3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indxr = VectorIndexer()\\\n",
    "  .setInputCol(\"features\")\\\n",
    "  .setOutputCol(\"idxed\")\\\n",
    "  .setMaxCategories(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indxr.fit(idxIn).transform(idxIn).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|color|colorInd|\n",
      "+-----+--------+\n",
      "|green|     1.0|\n",
      "| blue|     2.0|\n",
      "| blue|     2.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colorLab.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------------------------------+\n",
      "|color|colorInd|OneHotEncoder_4ace9dd77f85339340be__output|\n",
      "+-----+--------+------------------------------------------+\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "+-----+--------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setToLowercase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setGaps(False)\\\n",
    "  .setToLowercase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------+\n",
      "|Description                        |DescOut           |\n",
      "+-----------------------------------+------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
      "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
      "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
      "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
      "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
      "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
      "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
      "|POPCORN HOLDER                     |[ ]               |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
      "+-----------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "stops = StopWordsRemover()\\\n",
    "  .setStopWords(englishStopWords)\\\n",
    "  .setInputCol(\"DescOut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "condi = (col(\"Description\") == 'SET OF 4 KNICK KNACK TINS POPPIES') | (col(\"Description\") == 'GROW A FLYTRAP OR SUNFLOWER IN TIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<((Description = SET OF 4 KNICK KNACK TINS POPPIES) OR (Description = GROW A FLYTRAP OR SUNFLOWER IN TIN))>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------------------------------+\n",
      "|DescOut                                   |StopWordsRemover_4a2bbde8faf8b270ceac__output|\n",
      "+------------------------------------------+---------------------------------------------+\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[grow, a, flytrap, or, sunflower, in, tin]|[grow, flytrap, sunflower, tin]              |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "|[grow, a, flytrap, or, sunflower, in, tin]|[grow, flytrap, sunflower, tin]              |\n",
      "|[grow, a, flytrap, or, sunflower, in, tin]|[grow, flytrap, sunflower, tin]              |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |[set, 4, knick, knack, tins, poppies]        |\n",
      "+------------------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stops.transform(tokenized)\\\n",
    ".select(\"*\",condi.alias(\"filtered\"))\\\n",
    ".where(\"filtered\")\\\n",
    ".drop(\"Description\",\"filtered\")\\\n",
    ".show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_4969b5c9149c0f2fb385__output   |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue, harmonica, in, box]           |\n",
      "|[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls, , water, transfer, tattoos] |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft, girl, amelie, kit]       |\n",
      "|[camouflage, led, torch]             |[camouflage, led, torch]             |\n",
      "|[white, skull, hot, water, bottle]   |[white, skull, hot, water, bottle]   |\n",
      "|[english, rose, hot, water, bottle]  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram.transform(tokenized.select(\"DescOut\")).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_4c60ad3e57de70c4af3d__output                     |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue harmonica, harmonica in, in box]                 |\n",
      "|[gumball, coat, rack]                |[gumball coat, coat rack]                              |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls ,  water, water transfer, transfer tattoos]    |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft girl, girl amelie, amelie kit]              |\n",
      "|[camouflage, led, torch]             |[camouflage led, led torch]                            |\n",
      "|[white, skull, hot, water, bottle]   |[white skull, skull hot, hot water, water bottle]      |\n",
      "|[english, rose, hot, water, bottle]  |[english rose, rose hot, hot water, water bottle]      |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram.transform(tokenized.select(\"DescOut\")).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"countVec\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(1)\\\n",
    "  .setMinDF(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------------------------------------+\n",
      "|DescOut                                   |countVec                                           |\n",
      "+------------------------------------------+---------------------------------------------------+\n",
      "|[rabbit, night, light]                    |(500,[149,185,212],[1.0,1.0,1.0])                  |\n",
      "|[doughnut, lip, gloss]                    |(500,[462,463,491],[1.0,1.0,1.0])                  |\n",
      "|[12, message, cards, with, envelopes]     |(500,[35,41,166],[1.0,1.0,1.0])                    |\n",
      "|[blue, harmonica, in, box]                |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])             |\n",
      "|[gumball, coat, rack]                     |(500,[228,281,407],[1.0,1.0,1.0])                  |\n",
      "|[skulls, , water, transfer, tattoos]      |(500,[11,40,133],[1.0,1.0,1.0])                    |\n",
      "|[feltcraft, girl, amelie, kit]            |(500,[60,64,69],[1.0,1.0,1.0])                     |\n",
      "|[camouflage, led, torch]                  |(500,[264],[1.0])                                  |\n",
      "|[white, skull, hot, water, bottle]        |(500,[15,34,39,40,118],[1.0,1.0,1.0,1.0,1.0])      |\n",
      "|[english, rose, hot, water, bottle]       |(500,[34,39,40,46,169],[1.0,1.0,1.0,1.0,1.0])      |\n",
      "|[hot, water, bottle, keep, calm]          |(500,[34,39,40,147,148],[1.0,1.0,1.0,1.0,1.0])     |\n",
      "|[scottie, dog, hot, water, bottle]        |(500,[34,39,40,146,386],[1.0,1.0,1.0,1.0,1.0])     |\n",
      "|[rose, caravan, doorstop]                 |(500,[46,297],[1.0,1.0])                           |\n",
      "|[gingham, heart, , doorstop, red]         |(500,[3,4,11,143,297],[1.0,1.0,1.0,1.0,1.0])       |\n",
      "|[storage, tin, vintage, leaf]             |(500,[6,45,109,162],[1.0,1.0,1.0,1.0])             |\n",
      "|[set, of, 4, knick, knack, tins, poppies] |(500,[0,1,49,70,365,366],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[popcorn, holder]                         |(500,[21,296],[1.0,1.0])                           |\n",
      "|[grow, a, flytrap, or, sunflower, in, tin]|(500,[36,45,378],[1.0,1.0,1.0])                    |\n",
      "|[airline, bag, vintage, world, champion]  |(500,[2,6,328],[1.0,1.0,1.0])                      |\n",
      "|[airline, bag, vintage, jet, set, brown]  |(500,[0,2,6,328,405],[1.0,1.0,1.0,1.0,1.0])        |\n",
      "+------------------------------------------+---------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).drop(\"Description\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "tfIdfIn = tokenized\\\n",
    "  .where(\"array_contains(DescOut, 'red')\")\\\n",
    "  .select(\"DescOut\")\\\n",
    "  .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|DescOut                               |\n",
      "+--------------------------------------+\n",
      "|[wooden, advent, calendar, red]       |\n",
      "|[wrap, red, vintage, doily]           |\n",
      "|[alarm, clock, bakelike, red]         |\n",
      "|[alarm, clock, bakelike, red]         |\n",
      "|[red, diner, wall, clock]             |\n",
      "|[red, apples, chopping, board]        |\n",
      "|[red, kitchen, scales]                |\n",
      "|[airline, bag, vintage, jet, set, red]|\n",
      "|[red, retrospot, charlotte, bag]      |\n",
      "|[pin, cushion, babushka, red]         |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"TFOut\")\\\n",
    "  .setNumFeatures(10000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF()\\\n",
    "  .setInputCol(\"TFOut\")\\\n",
    "  .setOutputCol(\"IDFOut\")\\\n",
    "  .setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|TFOut                                                           |IDFOut                                                          |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|(10000,[829,2311,4291,8242],[1.0,1.0,1.0,1.0])                  |(10000,[829,2311,4291,8242],[0.0,0.0,0.0,0.0])                  |\n",
      "|(10000,[792,4291,8150,9749],[1.0,1.0,1.0,1.0])                  |(10000,[792,4291,8150,9749],[0.0,0.0,0.0,0.0])                  |\n",
      "|(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])                 |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                 |\n",
      "|(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])                 |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                 |\n",
      "|(10000,[4291,6404,8873,9668],[1.0,1.0,1.0,1.0])                 |(10000,[4291,6404,8873,9668],[0.0,0.0,0.0,0.0])                 |\n",
      "|(10000,[161,4291,4437,9509],[1.0,1.0,1.0,1.0])                  |(10000,[161,4291,4437,9509],[0.0,0.0,0.0,0.0])                  |\n",
      "|(10000,[3461,4291,6214],[1.0,1.0,1.0])                          |(10000,[3461,4291,6214],[0.0,0.0,0.0])                          |\n",
      "|(10000,[155,3948,4291,8150,8813,9924],[1.0,1.0,1.0,1.0,1.0,1.0])|(10000,[155,3948,4291,8150,8813,9924],[0.0,0.0,0.0,0.0,0.0,0.0])|\n",
      "|(10000,[155,2591,4291,4835],[1.0,1.0,1.0,1.0])                  |(10000,[155,2591,4291,4835],[0.0,1.0116009116784799,0.0,0.0])   |\n",
      "|(10000,[4291,5111,5673,7153],[1.0,1.0,1.0,1.0])                 |(10000,[4291,5111,5673,7153],[0.0,0.0,0.0,1.2992829841302609])  |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).drop(\"DescOut\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "  outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|text                                      |result                                                           |\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |[0.007668816111981869,-0.03736278116703034,0.017676191776990893] |\n",
      "|[I, wish, Java, could, use, case, classes]|[-0.017213788947888782,0.030650552528511198,0.046941662672907114]|\n",
      "|[Logistic, regression, models, are, neat] |[0.10106341168284416,-0.04297380540519953,0.005813928321003914]  |\n",
      "+------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.00766881611198,-0.037362781167,0.017676191777]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.0172137889479,0.0306505525285,0.0469416626729]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.101063411683,-0.0429738054052,0.005813928321]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_4a8195a7120ed05eba92__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "|id |features      |PolynomialExpansion_425abd60463c5192118f__output                                   |\n",
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|0  |[1.0,0.1,-1.0]|[1.0,1.0,0.1,0.1,0.010000000000000002,-1.0,-1.0,-0.1,1.0]                          |\n",
      "|1  |[2.0,1.1,1.0] |[2.0,4.0,1.1,2.2,1.2100000000000002,1.0,2.0,1.1,1.0]                               |\n",
      "|1  |[3.0,10.1,3.0]|[3.0,9.0,10.1,30.299999999999997,102.00999999999999,3.0,9.0,30.299999999999997,9.0]|\n",
      "+---+--------------+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = tkn\\\n",
    "  .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    "  .where(\"CustomerId IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+-------------------------------------+\n",
      "|Description                    |CustomerId|DescOut                              |\n",
      "+-------------------------------+----------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |14075.0   |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |14075.0   |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|14075.0   |[12, message, cards, with, envelopes]|\n",
      "+-------------------------------+----------+-------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prechi = fittedCV.transform(tokenized)\\\n",
    "  .where(\"CustomerId IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+-------------------------------------+---------------------------------+\n",
      "|Description                    |CustomerId|DescOut                              |countVec                         |\n",
      "+-------------------------------+----------+-------------------------------------+---------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |14075.0   |[rabbit, night, light]               |(500,[149,185,212],[1.0,1.0,1.0])|\n",
      "|DOUGHNUT LIP GLOSS             |14075.0   |[doughnut, lip, gloss]               |(500,[462,463,491],[1.0,1.0,1.0])|\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|14075.0   |[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])  |\n",
      "+-------------------------------+----------+-------------------------------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prechi.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chisq = ChiSqSelector()\\\n",
    "  .setFeaturesCol(\"countVec\")\\\n",
    "  .setLabelCol(\"CustomerId\")\\\n",
    "  .setNumTopFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------------------+\n",
      "|            countVec|ChiSqSelector_45949a22610dcebde5eb__output|\n",
      "+--------------------+------------------------------------------+\n",
      "|(500,[149,185,212...|                                 (2,[],[])|\n",
      "|(500,[462,463,491...|                                 (2,[],[])|\n",
      "|(500,[35,41,166],...|                                 (2,[],[])|\n",
      "|(500,[10,16,36,35...|                                 (2,[],[])|\n",
      "|(500,[228,281,407...|                                 (2,[],[])|\n",
      "|(500,[11,40,133],...|                                 (2,[],[])|\n",
      "|(500,[60,64,69],[...|                                 (2,[],[])|\n",
      "|   (500,[264],[1.0])|                                 (2,[],[])|\n",
      "|(500,[15,34,39,40...|                                 (2,[],[])|\n",
      "|(500,[34,39,40,46...|                                 (2,[],[])|\n",
      "|(500,[34,39,40,14...|                                 (2,[],[])|\n",
      "|(500,[34,39,40,14...|                                 (2,[],[])|\n",
      "|(500,[46,297],[1....|                                 (2,[],[])|\n",
      "|(500,[3,4,11,143,...|                                 (2,[],[])|\n",
      "|(500,[6,45,109,16...|                                 (2,[],[])|\n",
      "|(500,[0,1,49,70,3...|                       (2,[0,1],[1.0,1.0])|\n",
      "|(500,[21,296],[1....|                                 (2,[],[])|\n",
      "|(500,[36,45,378],...|                                 (2,[],[])|\n",
      "|(500,[2,6,328],[1...|                                 (2,[],[])|\n",
      "|(500,[0,2,6,328,4...|                             (2,[0],[1.0])|\n",
      "+--------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " chisq.fit(prechi).transform(prechi)\\\n",
    "    .drop(\"customerId\", \"Description\", \"DescOut\").show()\n",
    "\n",
    "# Py4JJavaError: An error occurred while calling o929.fit.\n",
    "# : java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
    "# .config(\"spark.driver.memory\", \"8g\")\\\n",
    "# .config(\"spark.executor.memory\", \"3g\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/user/kranthidr/savedModels/fittedPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_4a8195a7120ed05eba92__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"/user/kranthidr/savedModels/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark2.7_home",
   "language": "python",
   "name": "pyspark2.7_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
