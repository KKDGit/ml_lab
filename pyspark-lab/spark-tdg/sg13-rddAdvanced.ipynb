{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/hdp/current/spark2-client'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "#findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"sg13-rddAdvanced\").getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local[*]\").appName(\"sg13-rddAdvanced\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://rm01.itversity.com:19288/proxy/application_1533622723243_16447\n"
     ]
    }
   ],
   "source": [
    "for x in sc._conf.getAll():\n",
    "    if '/proxy/' in x[1]:\n",
    "        print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "words.map(lambda word: (word.lower(), 1))\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword = words.keyBy(lambda word: word.lower()[0])\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.mapValues(lambda word: word.upper()).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.flatMapValues(lambda word: word.upper()).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyword.keys().collect()\n",
    "keyword.values().collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct()\\\n",
    "  .collect()\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "words.map(lambda word: (word.lower()[0], word))\\\n",
    "  .sampleByKey(True, sampleMap, 6).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "chars = words.flatMap(lambda word: word.lower())\n",
    "KVcharacters = chars.map(lambda letter: (letter, 1))\n",
    "def maxFunc(left, right):\n",
    "  return max(left, right)\n",
    "def addFunc(left, right):\n",
    "  return left + right\n",
    "nums = sc.parallelize(range(1,31), 5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.countByKey()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1])))\\\n",
    "  .collect()\n",
    "# note this is Python 2, reduce must be imported from functools in Python 3\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "nums.aggregate(0, maxFunc, addFunc)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "depth = 3\n",
    "nums.treeAggregate(0, maxFunc, addFunc, depth)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.aggregateByKey(0, addFunc, maxFunc).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def valToCombiner(value):\n",
    "  return [value]\n",
    "def mergeValuesFunc(vals, valToAppend):\n",
    "  vals.append(valToAppend)\n",
    "  return vals\n",
    "def mergeCombinerFunc(vals1, vals2):\n",
    "  return vals1 + vals2\n",
    "outputPartitions = 6\n",
    "KVcharacters\\\n",
    "  .combineByKey(\n",
    "    valToCombiner,\n",
    "    mergeValuesFunc,\n",
    "    mergeCombinerFunc,\n",
    "    outputPartitions)\\\n",
    "  .collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "KVcharacters.foldByKey(0, addFunc).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import random\n",
    "distinctChars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "charRDD = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD2 = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD.cogroup(charRDD2).take(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "keyedChars = distinctChars.map(lambda c: (c, random.random()))\n",
    "outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "numRange = sc.parallelize(range(10), 2)\n",
    "words.zip(numRange).collect()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "words.coalesce(1).getNumPartitions() # 1\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/data/retail-data/all/\")\n",
    "rdd = df.coalesce(10).rdd\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def partitionFunc(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    return 0\n",
    "  else:\n",
    "    return random.randint(1,2)\n",
    "\n",
    "keyedRDD = rdd.keyBy(lambda row: row[6])\n",
    "keyedRDD\\\n",
    "  .partitionBy(3, partitionFunc)\\\n",
    "  .map(lambda x: x[0])\\\n",
    "  .glom()\\\n",
    "  .map(lambda x: len(set(x)))\\\n",
    "  .take(5)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark2.7_home",
   "language": "python",
   "name": "pyspark2.7_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
