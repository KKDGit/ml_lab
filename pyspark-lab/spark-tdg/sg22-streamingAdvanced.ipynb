{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform,os\n",
    "os_name = platform.system()\n",
    "hdfs_home = \"/user/\" + os.getenv(\"HOME\").split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/kranthidr5051\n",
      "Linux\n"
     ]
    }
   ],
   "source": [
    "print(hdfs_home)\n",
    "print(os_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = hdfs_home+\"/dataSets/spark-guide/activity-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = \"/usr/hdp/current/spark2-client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/hdp/current/spark2-client'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(spark_home)\n",
    "#findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"sg22-streamingAdvanced\").getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local[*]\").appName(\"sg22-streamingAdvanced\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ip-172-31-35-141.ec2.internal:8088/proxy/application_1534772926501_6712\n"
     ]
    }
   ],
   "source": [
    "for x in sc._conf.getAll():\n",
    "    if '/proxy/' in x[1]:\n",
    "        print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark\\\n",
    "  .readStream\\\n",
    "  .schema(static.schema)\\\n",
    "  .option(\"maxFilesPerTrigger\", 10)\\\n",
    "  .json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|Arrival_Time |Creation_Time      |Device  |Index|Model |User|gt   |x           |y           |z           |\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|18   |nexus4|g   |stand|3.356934E-4 |-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|66   |nexus4|g   |stand|-0.005722046|0.029083252 |0.005569458 |\n",
      "|1424686735500|1424686733498505625|nexus4_1|99   |nexus4|g   |stand|0.0078125   |-0.017654419|0.010025024 |\n",
      "|1424686735691|1424688581745026978|nexus4_2|145  |nexus4|g   |stand|-3.814697E-4|0.0184021   |-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|185  |nexus4|g   |stand|-3.814697E-4|-0.031799316|-0.00831604 |\n",
      "|1424686736094|1424686734097840342|nexus4_1|218  |nexus4|g   |stand|-7.324219E-4|-0.013381958|0.01109314  |\n",
      "|1424686736294|1424688582347932252|nexus4_2|265  |nexus4|g   |stand|-0.005722046|0.015197754 |0.022659302 |\n",
      "|1424686736495|1424688582549592408|nexus4_2|305  |nexus4|g   |stand|-3.814697E-4|0.0087890625|0.0034332275|\n",
      "|1424686736697|1424688582750703248|nexus4_2|345  |nexus4|g   |stand|0.002822876 |-0.008300781|-0.015792847|\n",
      "|1424686736898|1424688582952241334|nexus4_2|385  |nexus4|g   |stand|6.866455E-4 |-0.008300781|0.004501343 |\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|event_time            |\n",
      "+----------------------+\n",
      "|1.42468673309063808E18|\n",
      "|1.42468858134591821E18|\n",
      "|1.42468673349850573E18|\n",
      "|1.42468858174502707E18|\n",
      "|1.42468858194525286E18|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.selectExpr(\"cast(Creation_Time as double) as event_time\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|event_time          |\n",
      "+--------------------+\n",
      "|1.4246867330906382E9|\n",
      "|1.4246885813459182E9|\n",
      "|1.4246867334985058E9|\n",
      "|1.424688581745027E9 |\n",
      "|1.424688581945253E9 |\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.selectExpr(\"cast(Creation_Time as double)/1000000000 as event_time\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|event_time                |\n",
      "+--------------------------+\n",
      "|2015-02-23 10:18:53.090638|\n",
      "|2015-02-23 10:49:41.345918|\n",
      "|2015-02-23 10:18:53.498505|\n",
      "|2015-02-23 10:49:41.745027|\n",
      "|2015-02-23 10:49:41.945253|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.selectExpr(\"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "withEventTime = streaming.selectExpr(\n",
    "  \"*\",\n",
    "  \"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3d8c2f1d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"pyevents_per_window\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|window|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|window|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|37714|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|33324|\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|25218|\n",
      "|[2015-02-23 10:20:00, 2015-02-23 10:30:00]|24856|\n",
      "|[2015-02-24 12:30:00, 2015-02-24 12:40:00]|31496|\n",
      "+------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM pyevents_per_window\").show(5, False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f3d8c181bd0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3d84cec910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "  .count()\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"pyevents_per_window1\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|window|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2015-02-23 14:15:00, 2015-02-23 14:25:00]|13523|\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|18854|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|16636|\n",
      "|[2015-02-22 00:35:00, 2015-02-22 00:45:00]|5    |\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|12617|\n",
      "+------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2015-02-23 14:15:00, 2015-02-23 14:25:00]|26936|\n",
      "|[2015-02-24 11:50:00, 2015-02-24 12:00:00]|37714|\n",
      "|[2015-02-24 13:00:00, 2015-02-24 13:10:00]|33324|\n",
      "|[2015-02-22 00:35:00, 2015-02-22 00:45:00]|6    |\n",
      "|[2015-02-23 12:30:00, 2015-02-23 12:40:00]|25218|\n",
      "+------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM pyevents_per_window1\").show(5, False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f3d8c181150>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f3d8c181d10>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f3d84cec650>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "withEventTime\\\n",
    "  .withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "  .groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    "  .count()\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"pyevents_per_window2\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|window|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM pyevents_per_window2\").show(5, False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "withEventTime\\\n",
    "  .withWatermark(\"event_time\", \"5 seconds\")\\\n",
    "  .dropDuplicates([\"User\", \"event_time\"])\\\n",
    "  .groupBy(\"User\")\\\n",
    "  .count()\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"pydeduplicated\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM pydeduplicated\").show(5, False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark2.7_home",
   "language": "python",
   "name": "pyspark2.7_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
