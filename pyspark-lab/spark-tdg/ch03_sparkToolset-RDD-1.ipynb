{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"myAppName\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('data/auto_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18,8,307,130,3504,12,70,1',\n",
       " '15,8,350,165,3693,11.5,70,1',\n",
       " '18,8,318,150,3436,11,70,1',\n",
       " '16,8,304,150,3433,12,70,1',\n",
       " '17,8,302,140,3449,10.5,70,1',\n",
       " '15,8,429,198,4341,10,70,1',\n",
       " '14,8,454,220,4354,9,70,1',\n",
       " '14,8,440,215,4312,8.5,70,1',\n",
       " '14,8,455,225,4425,10,70,1',\n",
       " '15,8,390,190,3850,8.5,70,1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data.map(lambda x:Row(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "raw_dataDF = raw_data.map(lambda x:Row(x)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "|18,8,307,130,3504...|\n",
      "|15,8,350,165,3693...|\n",
      "|18,8,318,150,3436...|\n",
      "|16,8,304,150,3433...|\n",
      "|17,8,302,140,3449...|\n",
      "|15,8,429,198,4341...|\n",
      "|14,8,454,220,4354...|\n",
      "|14,8,440,215,4312...|\n",
      "|14,8,455,225,4425...|\n",
      "|15,8,390,190,3850...|\n",
      "|15,8,383,170,3563...|\n",
      "|14,8,340,160,3609...|\n",
      "|15,8,400,150,3761...|\n",
      "|14,8,455,225,3086...|\n",
      "|24,4,113,95,2372,...|\n",
      "|22,6,198,95,2833,...|\n",
      "|18,6,199,97,2774,...|\n",
      "|21,6,200,85,2587,...|\n",
      "|27,4,97,88,2130,1...|\n",
      "|26,4,97,46,1835,2...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (raw_data\n",
    "        .map(lambda x:map(str, x.split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertMaptoRDD(map_ele):\n",
    "    l = [str(x) for x in map_ele]\n",
    "    print(l)\n",
    "    return Row(l[0],l[1],l[2],l[3],l[4],l[5],l[6],l[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = data.take(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18', '8', '307', '130', '3504', '12', '70', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Row(18, 8, 307, 130, 3504, 12, 70, 1)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertMaptoRDD(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRow = data.map(lambda x:convertMaptoRDD(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(18, 8, 307, 130, 3504, 12, 70, 1)>,\n",
       " <Row(15, 8, 350, 165, 3693, 11.5, 70, 1)>,\n",
       " <Row(18, 8, 318, 150, 3436, 11, 70, 1)>,\n",
       " <Row(16, 8, 304, 150, 3433, 12, 70, 1)>,\n",
       " <Row(17, 8, 302, 140, 3449, 10.5, 70, 1)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRow.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"1\", StringType(), False),\n",
    "    StructField(\"2\", StringType(), False),\n",
    "    StructField(\"3\", StringType(), False),\n",
    "    StructField(\"4\", StringType(), False),\n",
    "    StructField(\"5\", StringType(), False),\n",
    "    StructField(\"6\", StringType(), False),\n",
    "    StructField(\"7\", StringType(), False),\n",
    "    StructField(\"8\", StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRowDF = spark.createDataFrame(dataRow, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+----+---+---+\n",
      "|  1|  2|  3|  4|   5|   6|  7|  8|\n",
      "+---+---+---+---+----+----+---+---+\n",
      "| 18|  8|307|130|3504|  12| 70|  1|\n",
      "| 15|  8|350|165|3693|11.5| 70|  1|\n",
      "| 18|  8|318|150|3436|  11| 70|  1|\n",
      "| 16|  8|304|150|3433|  12| 70|  1|\n",
      "| 17|  8|302|140|3449|10.5| 70|  1|\n",
      "| 15|  8|429|198|4341|  10| 70|  1|\n",
      "| 14|  8|454|220|4354|   9| 70|  1|\n",
      "| 14|  8|440|215|4312| 8.5| 70|  1|\n",
      "| 14|  8|455|225|4425|  10| 70|  1|\n",
      "| 15|  8|390|190|3850| 8.5| 70|  1|\n",
      "| 15|  8|383|170|3563|  10| 70|  1|\n",
      "| 14|  8|340|160|3609|   8| 70|  1|\n",
      "| 15|  8|400|150|3761| 9.5| 70|  1|\n",
      "| 14|  8|455|225|3086|  10| 70|  1|\n",
      "| 24|  4|113| 95|2372|  15| 70|  3|\n",
      "| 22|  6|198| 95|2833|15.5| 70|  1|\n",
      "| 18|  6|199| 97|2774|15.5| 70|  1|\n",
      "| 21|  6|200| 85|2587|  16| 70|  1|\n",
      "| 27|  4| 97| 88|2130|14.5| 70|  3|\n",
      "| 26|  4| 97| 46|1835|20.5| 70|  2|\n",
      "+---+---+---+---+----+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRowDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x199f26a6a20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize([Row(1), Row(2), Row(3)]).toDF().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-lab",
   "language": "python",
   "name": "pyspark-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
