{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-hadoop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.driver.memory\", \"28g\")\\\n",
    ".config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11\")\\\n",
    ".appName(\"ch31_DeepLearningLappy-docs\").getOrCreate()\n",
    "\n",
    "#.config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11,databricks:tensorframes:0.5.0-s_2.11\")\\\n",
    "# .config(\"spark.driver.memory\", \"8g\")\\\n",
    "# .config(\"spark.executor.memory\", \"3g\")\\\n",
    "# .config(\"spark.executor.cores\", \"4\")\\\n",
    "# .config(\"spark.executor.instances\", \"10\")\\\n",
    "\n",
    "\n",
    "# This file is sourced when running various Spark programs.\n",
    "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
    "# Options read in YARN client mode\n",
    "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n",
    "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n",
    "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n",
    "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINDOWS-JVACTDB:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch31_DeepLearningLappy-docs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24e503de2e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sparkdl.image import imageIO\n",
    "from pyspark.ml.image import ImageSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "img_dir = \"D:/Learn/GitRepos/Spark-The-Definitive-Guide/data/deep-learning-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tulips_df = ImageSchema.readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\n",
    "tulips_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[16]:\n",
    "daisy_df = imageIO.readImagesWithCustomFn(img_dir + \"/daisy\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(0))\n",
    "daisy_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "tulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\n",
    "daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "train_images_df = tulips_train.unionAll(daisy_train).cache()\n",
    "test_images_df = tulips_test.unionAll(daisy_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(train_images_df.count())\n",
    "print(test_images_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "model = p.fit(train_images_df)    # train_images_df is a dataset of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training error\n",
    "df = model.transform(train_images_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = false)\n",
      " |    |-- width: integer (nullable = false)\n",
      " |    |-- nChannels: integer (nullable = false)\n",
      " |    |-- mode: integer (nullable = false)\n",
      " |    |-- data: binary (nullable = false)\n",
      " |-- label: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = df.select(\"prediction\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Training set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect training error\n",
    "df = model.transform(test_images_df.limit(10))\n",
    "predictionAndLabels = df.select(\"prediction\", \"label\")\n",
    "print(predictionAndLabels.count())\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights=\"imagenet\")\n",
    "model.save(\"D:/tmp/model-full-tmp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sparkdl.estimators.keras_image_file_estimator import KerasImageFileEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_uri(local_uri):\n",
    "  img = (PIL.Image.open(local_uri).convert('RGB').resize((299, 299), PIL.Image.ANTIALIAS))\n",
    "  img_arr = np.array(img).astype(np.float32)\n",
    "  img_tnsr = preprocess_input(img_arr[np.newaxis, :])\n",
    "  return img_tnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasImageFileEstimator( inputCol=\"uri\",\n",
    "                                     outputCol=\"prediction\",\n",
    "                                     labelCol=\"one_hot_label\",\n",
    "                                     imageLoader=load_image_from_uri,\n",
    "                                     kerasOptimizer='adam',\n",
    "                                     kerasLoss='categorical_crossentropy', # why categorical_crossentropy\n",
    "                                     modelFile=\"D:/tmp/model-full-tmp.h5\" # local file path for model\n",
    "                                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "  ParamGridBuilder()\n",
    "  .addGrid(estimator.kerasFitParams, [{\"batch_size\": 32, \"verbose\": 0},\n",
    "                                      {\"batch_size\": 64, \"verbose\": 0}])\n",
    "  .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\" )\n",
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=bc, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.linalg as spla\n",
    "import pyspark.sql.types as sptyp\n",
    "\n",
    "import os\n",
    "#from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTrainImageUriandLabels(image_uris, label, cardinality):\n",
    "  # Create image categorical labels (integer IDs)\n",
    "  local_rows = []\n",
    "  for uri in image_uris:\n",
    "    label_inds = np.zeros(cardinality)\n",
    "    label_inds[label] = 1.0\n",
    "    one_hot_vec = spla.Vectors.dense(label_inds.tolist())\n",
    "    _row_struct = {\"uri\": uri, \"one_hot_label\": one_hot_vec, \"label\": float(label)}\n",
    "    row = sptyp.Row(**_row_struct)\n",
    "    local_rows.append(row)\n",
    "\n",
    "  image_uri_df = spark.createDataFrame(local_rows)\n",
    "  return image_uri_df\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "   for dirpath,_,filenames in os.walk(directory):\n",
    "       for f in filenames:\n",
    "           yield os.path.abspath(os.path.join(dirpath, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object absoluteFilePaths at 0x0000024E3C1592B0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absoluteFilePaths(img_dir + \"/tulips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cardinality = 2\n",
    "\n",
    "tulips_files = [str(f) for f in absoluteFilePaths(img_dir + \"/tulips\")]  # make \"local\" file paths for images\n",
    "tulips_uri_df = CreateTrainImageUriandLabels(tulips_files,1,label_cardinality)\n",
    "daisy_files = [str(f) for f in absoluteFilePaths(img_dir + \"/daisy\")]  # make \"local\" file paths for images\n",
    "daisy_uri_df = CreateTrainImageUriandLabels(daisy_files,0,label_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tulips_train, tulips_test, _ = tulips_uri_df.randomSplit([0.005, 0.005, 0.99]) \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "tulips_train, tulips_test = tulips_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daisy_train, daisy_test, _ = daisy_uri_df.randomSplit([0.005, 0.005, 0.99])    \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "daisy_train, daisy_test = daisy_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tulips_train.unionAll(daisy_train).cache()\n",
    "test_df = tulips_test.unionAll(daisy_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+\n",
      "|label|one_hot_label|                 uri|\n",
      "+-----+-------------+--------------------+\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "|  1.0|    [0.0,1.0]|D:\\Learn\\GitRepos...|\n",
      "+-----+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# This ensure that each of the paritions has a small size.\n",
    "# train_df = train_df.repartition(100)\n",
    "# test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel = cv.fit(train_df)\n",
    "\n",
    "# #\n",
    "# File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 137, in standardize_input_data\n",
    "#     str(data_shape))\n",
    "# ValueError: Error when checking target: expected predictions to have shape (1000,) but got array with shape (2,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "from sparkdl import DeepImagePredictor\n",
    "\n",
    "image_df = ImageSchema.readImages(sample_img_dir)\n",
    "\n",
    "predictor = DeepImagePredictor(inputCol=\"image\", \n",
    "                               outputCol=\"predicted_labels\", \n",
    "                               modelName=\"InceptionV3\", \n",
    "                               decodePredictions=True, \n",
    "                               topK=10)\n",
    "\n",
    "predictions_df = predictor.transform(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.types import StringType\n",
    "from sparkdl import KerasImageFileTransformer\n",
    "\n",
    "def loadAndPreprocessKerasInceptionV3(uri):\n",
    "  # this is a typical way to load and prep images in keras\n",
    "  image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n",
    "  image = np.expand_dims(image, axis=0)\n",
    "  return preprocess_input(image)\n",
    "\n",
    "transformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n",
    "                                        modelFile='/tmp/model-full-tmp.h5',  # local file path for model\n",
    "                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n",
    "                                        outputMode=\"vector\")\n",
    "\n",
    "files = [os.path.abspath(os.path.join(dirpath, f)) for f in os.listdir(\"/data/myimages\") if f.endswith('.jpg')]\n",
    "uri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"uri\")\n",
    "\n",
    "keras_pred_df = transformer.transform(uri_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import KerasTransformer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Generate random input data\n",
    "num_features = 10\n",
    "num_examples = 100\n",
    "input_data = [{\"features\" : np.random.randn(num_features).tolist()} for i in range(num_examples)]\n",
    "input_df = sqlContext.createDataFrame(input_data)\n",
    "\n",
    "# Create and save a single-hidden-layer Keras model for binary classification\n",
    "# NOTE: In a typical workflow, we'd train the model before exporting it to disk,\n",
    "# but we skip that step here for brevity\n",
    "model = Sequential()\n",
    "model.add(Dense(units=20, input_shape=[num_features], activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model_path = \"/tmp/simple-binary-classification\"\n",
    "model.save(model_path)\n",
    "\n",
    "# Create transformer and apply it to our input data\n",
    "transformer = KerasTransformer(inputCol=\"features\", outputCol=\"predictions\", modelFile=model_path)\n",
    "final_df = transformer.transform(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
    "\n",
    "registerKerasImageUDF(\"inceptionV3_udf\", InceptionV3(weights=\"imagenet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registerKerasImageUDF(\"my_custom_keras_model_udf\", \"/tmp/model-full-tmp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
    "\n",
    "def keras_load_img(fpath):\n",
    "    from keras.preprocessing.image import load_img, img_to_array\n",
    "    import numpy as np\n",
    "    img = load_img(fpath, target_size=(299, 299))\n",
    "    return img_to_array(img).astype(np.uint8)\n",
    "\n",
    "registerKerasImageUDF(\"inceptionV3_udf_with_preprocessing\", InceptionV3(weights=\"imagenet\"), keras_load_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "image_df = ImageSchema.readImages(sample_img_dir)\n",
    "image_df.registerTempTable(\"sample_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT my_custom_keras_model_udf(image) as predictions from sample_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-lab",
   "language": "python",
   "name": "pyspark-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
