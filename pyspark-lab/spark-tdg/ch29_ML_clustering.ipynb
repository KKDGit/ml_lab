{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/hdp/current/spark2-client'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/hdp/current/spark2-client')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    ".appName(\"ch29Clustering\").getOrCreate()\n",
    "\n",
    "#.config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11,databricks:tensorframes:0.5.0-s_2.11\")\\\n",
    "# .config(\"spark.driver.memory\", \"1g\")\\\n",
    "# .config(\"spark.executor.memory\", \"3g\")\\\n",
    "# .config(\"spark.executor.cores\", \"4\")\\\n",
    "# .config(\"spark.executor.instances\", \"4\")\\\n",
    "\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This file is sourced when running various Spark programs.\n",
    "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
    "\n",
    "# Options read in YARN client mode\n",
    "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n",
    "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n",
    "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n",
    "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n",
    "#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\n",
    "#SPARK_YARN_QUEUE=\"default\" #The hadoop queue to use for allocation requests (Default: default)\n",
    "#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\n",
    "#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://rm01.itversity.com:19288/proxy/application_1533622723243_18510\n"
     ]
    }
   ],
   "source": [
    "for x in sc._conf.getAll():\n",
    "    if \"/proxy/\" in x[1]:\n",
    "        print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/user/kranthidr/dataSets/spark-guide/\"\n",
    "path = data_dir + \"retail-data/by-day/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = va.transform(spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(path)\n",
    "  .limit(50)\n",
    "  .coalesce(4)\n",
    "  .where(\"Description IS NOT NULL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|   features|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   573319|    23200|     JUMBO BAG PEARS|      20|2011-10-30 10:18:00|     2.08|   17651.0|United Kingdom|[20.0,2.08]|\n",
      "|   573319|    23203|JUMBO BAG VINTAGE...|      10|2011-10-30 10:18:00|     2.08|   17651.0|United Kingdom|[10.0,2.08]|\n",
      "|   573319|    22947|WOODEN ADVENT CAL...|       6|2011-10-30 10:18:00|     7.95|   17651.0|United Kingdom| [6.0,7.95]|\n",
      "|   573319|    21262|WHITE GOOSE FEATH...|       6|2011-10-30 10:18:00|     2.95|   17651.0|United Kingdom| [6.0,2.95]|\n",
      "|   573319|    21261|GREEN GOOSE FEATH...|       6|2011-10-30 10:18:00|     2.95|   17651.0|United Kingdom| [6.0,2.95]|\n",
      "|   573319|   84341B|SMALL PINK MAGIC ...|      36|2011-10-30 10:18:00|     0.38|   17651.0|United Kingdom|[36.0,0.38]|\n",
      "|   573319|    23581|JUMBO BAG PAISLEY...|      10|2011-10-30 10:18:00|     2.08|   17651.0|United Kingdom|[10.0,2.08]|\n",
      "|   573319|    23343|JUMBO BAG VINTAGE...|      10|2011-10-30 10:18:00|     2.08|   17651.0|United Kingdom|[10.0,2.08]|\n",
      "|   573319|    22720|SET OF 3 CAKE TIN...|       3|2011-10-30 10:18:00|     4.95|   17651.0|United Kingdom| [3.0,4.95]|\n",
      "|   573319|    22952|60 CAKE CASES VIN...|      24|2011-10-30 10:18:00|     0.55|   17651.0|United Kingdom|[24.0,0.55]|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler()\\\n",
    "  .setInputCols([\"Quantity\", \"UnitPrice\"])\\\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, features: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.clustering import KMeans\n",
    "km = KMeans().setK(5)\n",
    "\n",
    "# .. versionadded:: 1.5.0\n",
    "# Init docstring: __init__(self, \n",
    "#                          featuresCol=\"features\", predictionCol=\"prediction\", k=2,                  \n",
    "#                          initMode=\"k-means||\", initSteps=2, tol=1e-4, maxIter=20, seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -7649703878154674547)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "print km.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "summary = kmModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 12, 17, 11, 2]\n"
     ]
    }
   ],
   "source": [
    "print summary.clusterSizes # number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309.9689161987512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmModel.computeCost(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = kmModel.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 2.5     11.24375]\n",
      "[11.33333333  1.1       ]\n",
      "[4.88235294 3.95176471]\n",
      "[24.36363636  0.94636364]\n",
      "[48.    1.32]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkm = BisectingKMeans().setK(5).setMaxIter(5)\n",
    "bkmModel = bkm.fit(sales)\n",
    "\n",
    "# Init docstring: __init__(self, \n",
    "#                          featuresCol=\"features\", predictionCol=\"prediction\", maxIter=20,                  \n",
    "#                          seed=None, k=4, minDivisibleClusterSize=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "summary = bkmModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 16, 13, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "print summary.clusterSizes # number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309.9689161987512"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmModel.computeCost(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 2.5     11.24375]\n",
      "[11.33333333  1.1       ]\n",
      "[4.88235294 3.95176471]\n",
      "[24.36363636  0.94636364]\n",
      "[48.    1.32]\n"
     ]
    }
   ],
   "source": [
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gmm = GaussianMixture().setK(5)\n",
    "\n",
    "# Init docstring: __init__(self, \n",
    "#                          featuresCol=\"features\", predictionCol=\"prediction\", k=2,                  \n",
    "#                          probabilityCol=\"probability\", tol=0.01, maxIter=100, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuresCol: features column name. (default: features)\n",
      "k: Number of independent Gaussians in the mixture model. Must be > 1. (default: 2, current: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "seed: random seed. (default: -8733409933116740608)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.01)\n"
     ]
    }
   ],
   "source": [
    "print gmm.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gmm.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1809062831871321, 0.07999196088784609, 0.26043554453499296, 0.1999999297055438, 0.278666281684485]\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "summary = model.summary\n",
    "print model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                mean|                 cov|\n",
      "+--------------------+--------------------+\n",
      "|[6.64564026112604...|0.933953258142771...|\n",
      "|[1.99989950106789...|0.499949740950633...|\n",
      "|[27.9651724132995...|86.7354745185911 ...|\n",
      "|[12.0000000000027...|1.197804943230182...|\n",
      "|[3.64113652437146...|0.373621349812460...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.gaussiansDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         0|\n",
      "|         2|\n",
      "|         0|\n",
      "|         2|\n",
      "|         4|\n",
      "|         4|\n",
      "|         4|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         3|\n",
      "|         0|\n",
      "|         2|\n",
      "|         2|\n",
      "|         4|\n",
      "|         4|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 13, 10, 14]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.clusterSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[4.64645093967825...|\n",
      "|[4.42105054057479...|\n",
      "|[7.44248887777602...|\n",
      "|[3.14495872190316...|\n",
      "|[0.99997796565732...|\n",
      "|[3.37944930887881...|\n",
      "|[0.99999999999951...|\n",
      "|[8.43073641486631...|\n",
      "|[0.00752831083286...|\n",
      "|[0.00752831083286...|\n",
      "|[0.00227494800777...|\n",
      "|[2.45617379108935...|\n",
      "|[0.99361333095573...|\n",
      "|[0.99361333095573...|\n",
      "|[3.66470080547568...|\n",
      "|[0.99172939115085...|\n",
      "|[3.96818549340122...|\n",
      "|[3.14495872190316...|\n",
      "|[0.00752831083286...|\n",
      "|[0.00752831083286...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.probability.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.drop(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"features\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(0)\\\n",
    "  .setMinDF(0)\\\n",
    "  .setBinary(True)\n",
    "\n",
    "# Init docstring: __init__(self, \n",
    "#                          minTF=1.0, minDF=1.0, vocabSize=1 << 18, binary=False, inputCol=None,                 \n",
    "#                          outputCol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvFitted = cv.fit(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA().setK(10).setMaxIter(5)\n",
    "\n",
    "\n",
    "# Init docstring: __init__(self, \n",
    "#                          featuresCol=\"features\", maxIter=20, seed=None, checkpointInterval=10,                  \n",
    "#                          k=10, optimizer=\"online\", learningOffset=1024.0, learningDecay=0.51,                  \n",
    "#                          subsamplingRate=0.05, optimizeDocConcentration=True,                  \n",
    "#                          docConcentration=None, topicConcentration=None,                  \n",
    "#                          topicDistributionCol=\"topicDistribution\", keepLastCheckpoint=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "docConcentration: Concentration parameter (commonly named \"alpha\") for the prior placed on documents' distributions over topics (\"theta\"). (undefined)\n",
      "featuresCol: features column name. (default: features)\n",
      "k: The number of topics (clusters) to infer. Must be > 1. (default: 10, current: 10)\n",
      "keepLastCheckpoint: (For EM optimizer) If using checkpointing, this indicates whether to keep the last checkpoint. If false, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. (default: True)\n",
      "learningDecay: Learning rate, set as anexponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. (default: 0.51)\n",
      "learningOffset: A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less (default: 1024.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 5)\n",
      "optimizeDocConcentration: Indicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. (default: True)\n",
      "optimizer: Optimizer or inference algorithm used to estimate the LDA model.  Supported: online, em (default: online)\n",
      "seed: random seed. (default: 6653501418110561442)\n",
      "subsamplingRate: Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. (default: 0.05)\n",
      "topicConcentration: Concentration parameter (commonly named \"beta\" or \"eta\") for the prior placed on topic' distributions over terms. (undefined)\n",
      "topicDistributionCol: Output column with estimates of the topic mixture distribution for each document (often called \"theta\" in the literature). Returns a vector of zeros for an empty document. (default: topicDistribution)\n"
     ]
    }
   ],
   "source": [
    "print lda.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+\n",
      "|topic|    termIndices|         termWeights|\n",
      "+-----+---------------+--------------------+\n",
      "|    0|  [63, 113, 17]|[0.01164489032906...|\n",
      "|    1|   [10, 84, 32]|[0.00896269795055...|\n",
      "|    2|    [8, 14, 80]|[0.00906706283933...|\n",
      "|    3|  [16, 112, 82]|[0.00891670853537...|\n",
      "|    4|     [2, 4, 58]|[0.01771558726789...|\n",
      "|    5|    [9, 13, 66]|[0.01168320143030...|\n",
      "|    6|   [81, 94, 34]|[0.00879897934157...|\n",
      "|    7|[117, 131, 114]|[0.00869931727630...|\n",
      "|    8|   [71, 80, 47]|[0.01201947260119...|\n",
      "|    9|  [32, 42, 138]|[0.00919017536197...|\n",
      "+-----+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "model.describeTopics(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'water',\n",
       " u'hot',\n",
       " u'vintage',\n",
       " u'bottle',\n",
       " u'paperweight',\n",
       " u'6',\n",
       " u'home',\n",
       " u'doormat',\n",
       " u'landmark',\n",
       " u'bicycle',\n",
       " u'frame',\n",
       " u'ribbons',\n",
       " u'',\n",
       " u'classic',\n",
       " u'rose',\n",
       " u'kit',\n",
       " u'leaf',\n",
       " u'sweet',\n",
       " u'bag',\n",
       " u'airline',\n",
       " u'doorstop',\n",
       " u'light',\n",
       " u'in',\n",
       " u'christmas',\n",
       " u'heart',\n",
       " u'calm',\n",
       " u'set',\n",
       " u'keep',\n",
       " u'balloons',\n",
       " u'night',\n",
       " u'lights',\n",
       " u'12',\n",
       " u'tin',\n",
       " u'english',\n",
       " u'caravan',\n",
       " u'stuff',\n",
       " u'tidy',\n",
       " u'oxford',\n",
       " u'full',\n",
       " u'cottage',\n",
       " u'notting',\n",
       " u'drawer',\n",
       " u'mushrooms',\n",
       " u'chrome',\n",
       " u'champion',\n",
       " u'amelie',\n",
       " u'mini',\n",
       " u'the',\n",
       " u'giant',\n",
       " u'design',\n",
       " u'elegant',\n",
       " u'tins',\n",
       " u'jet',\n",
       " u'fairy',\n",
       " u\"50's\",\n",
       " u'holder',\n",
       " u'message',\n",
       " u'blue',\n",
       " u'storage',\n",
       " u'tier',\n",
       " u'covent',\n",
       " u'world',\n",
       " u'skulls',\n",
       " u'font',\n",
       " u'hearts',\n",
       " u'skull',\n",
       " u'clips',\n",
       " u'bell',\n",
       " u'red',\n",
       " u'party',\n",
       " u'chalkboard',\n",
       " u'save',\n",
       " u'4',\n",
       " u'coloured',\n",
       " u'poppies',\n",
       " u'garden',\n",
       " u'nine',\n",
       " u'girl',\n",
       " u'shimmering',\n",
       " u'doughnut',\n",
       " u'dog',\n",
       " u'3',\n",
       " u'tattoos',\n",
       " u'chilli',\n",
       " u'coat',\n",
       " u'torch',\n",
       " u'sunflower',\n",
       " u'tale',\n",
       " u'cards',\n",
       " u'puncture',\n",
       " u'woodland',\n",
       " u'bomb',\n",
       " u'knack',\n",
       " u'lip',\n",
       " u'collage',\n",
       " u'rabbit',\n",
       " u'sex',\n",
       " u'of',\n",
       " u'rack',\n",
       " u'wall',\n",
       " u'cracker',\n",
       " u'scottie',\n",
       " u'hill',\n",
       " u'led',\n",
       " u'black',\n",
       " u'art',\n",
       " u'envelopes',\n",
       " u'flytrap',\n",
       " u'box',\n",
       " u'pinks',\n",
       " u'camouflage',\n",
       " u'gingham',\n",
       " u'popcorn',\n",
       " u'with',\n",
       " u'knick',\n",
       " u'empire',\n",
       " u'grow',\n",
       " u'fancy',\n",
       " u'plate',\n",
       " u'natural',\n",
       " u'feltcraft',\n",
       " u'brown',\n",
       " u'paisley',\n",
       " u'repair',\n",
       " u'gumball',\n",
       " u'white',\n",
       " u'regency',\n",
       " u'cakestand',\n",
       " u'rocket',\n",
       " u'harmonica',\n",
       " u'a',\n",
       " u'or',\n",
       " u'transfer',\n",
       " u'street',\n",
       " u'planet',\n",
       " u'office',\n",
       " u'gloss',\n",
       " u'slate',\n",
       " u'towel',\n",
       " u'tea',\n",
       " u'breakfast']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvFitted.vocabulary\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark2.7_home",
   "language": "python",
   "name": "pyspark2.7_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
