{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-hadoop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    ".config(\"spark.driver.memory\", \"12g\")\\\n",
    ".config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11\")\\\n",
    ".appName(\"ch31_DeepLearningLappy-docs\").getOrCreate()\n",
    "\n",
    "#.config(\"spark.jars.packages\",\"databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11,databricks:tensorframes:0.5.0-s_2.11\")\\\n",
    "# .config(\"spark.driver.memory\", \"8g\")\\\n",
    "# .config(\"spark.executor.memory\", \"3g\")\\\n",
    "# .config(\"spark.executor.cores\", \"4\")\\\n",
    "# .config(\"spark.executor.instances\", \"10\")\\\n",
    "\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This file is sourced when running various Spark programs.\n",
    "# Copy it as spark-env.sh and edit that to configure Spark for your site.\n",
    "\n",
    "# Options read in YARN client mode\n",
    "#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n",
    "#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n",
    "#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n",
    "#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n",
    "#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\n",
    "#SPARK_YARN_QUEUE=\"default\" #The hadoop queue to use for allocation requests (Default: default)\n",
    "#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\n",
    "#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WINDOWS-JVACTDB:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ch31_DeepLearningLappy-docs</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ecd092d4a8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "img_dir = \"D:/Learn/GitRepos/Spark-The-Definitive-Guide/data/deep-learning-images\"\n",
    "img_dir_comb = \"D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\Combined30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights= None, classes=2)\n",
    "model.save(\"D:/tmp/model-full-tmp.h5\")\n",
    "\n",
    "# Signature: InceptionV3(include_top=True, weights='imagenet', \n",
    "#                        input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "# Docstring:\n",
    "# Instantiates the Inception v3 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(299, 299, 3)))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.save(\"D:/tmp/model-full-tmp1.h5\")  # saves to the local filesystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from os import listdir\n",
    "def absoluteFilePaths(directory):\n",
    "   for dirpath,_,filenames in os.walk(directory):\n",
    "       for f in filenames:\n",
    "           yield os.path.abspath(os.path.join(dirpath, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object absoluteFilePaths at 0x000001ED85D706D0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absoluteFilePaths(img_dir + \"/tulips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips_files = [str(f) for f in absoluteFilePaths(img_dir + \"/tulips\")]  # make \"local\" file paths for images\n",
    "daisy_files = [str(f) for f in absoluteFilePaths(img_dir + \"/daisy\")]  # make \"local\" file paths for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\tulips\\\\3498663243_42b39b4185_m.jpg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tulips_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\daisy\\\\10555815624_dc211569b0.jpg']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daisy_files[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.linalg as spla\n",
    "import pyspark.sql.types as sptyp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTrainImageUriandLabels(image_uris, label, cardinality):\n",
    "  # Create image categorical labels (integer IDs)\n",
    "  local_rows = []\n",
    "  for uri in image_uris:\n",
    "    label_inds = np.zeros(cardinality)\n",
    "    label_inds[label] = 1.0\n",
    "    one_hot_vec = spla.Vectors.dense(label_inds.tolist())\n",
    "    _row_struct = {\"uri\": uri, \"one_hot_label\": one_hot_vec, \"label\": float(label)}\n",
    "    row = sptyp.Row(**_row_struct)\n",
    "    local_rows.append(row)\n",
    "\n",
    "  image_uri_df = spark.createDataFrame(local_rows)\n",
    "  return image_uri_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cardinality = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips_uri_df = CreateTrainImageUriandLabels(tulips_files,1,label_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "daisy_uri_df = CreateTrainImageUriandLabels(daisy_files,0,label_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3498663243_42b39b4185_m.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3501996215_1c6d1a3386_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tulips_uri_df.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |[1.0,0.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\daisy\\10555815624_dc211569b0.jpg  |\n",
      "|0.0  |[1.0,0.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\daisy\\10555826524_423eb8bf71_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daisy_uri_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tulips_train, tulips_test, _ = tulips_uri_df.randomSplit([0.005, 0.005, 0.99]) \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "tulips_train, tulips_test = tulips_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daisy_train, daisy_test, _ = daisy_uri_df.randomSplit([0.005, 0.005, 0.99])    \n",
    "# use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)\n",
    "daisy_train, daisy_test = daisy_uri_df.randomSplit([0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tulips_train.unionAll(daisy_train).cache()\n",
    "test_df = tulips_test.unionAll(daisy_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3501996215_1c6d1a3386_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502085373_edc2c36992_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502251824_3be758edc6_m.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|label|one_hot_label|uri                                                                                                      |\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3498663243_42b39b4185_m.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502615974_ef4bd13202_n.jpg|\n",
      "|1.0  |[0.0,1.0]    |D:\\Learn\\GitRepos\\Spark-The-Definitive-Guide\\data\\deep-learning-images\\tulips\\3502632842_791dd4be18_n.jpg|\n",
      "+-----+-------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3, False)\n",
    "test_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sparkdl.estimators.keras_image_file_estimator import KerasImageFileEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_uri(local_uri):\n",
    "  img = (PIL.Image.open(local_uri).convert('RGB').resize((299, 299), PIL.Image.ANTIALIAS))\n",
    "  img_arr = np.array(img).astype(np.float32)\n",
    "  img_tnsr = preprocess_input(img_arr[np.newaxis, :])\n",
    "  return img_tnsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Learn\\\\GitRepos\\\\Spark-The-Definitive-Guide\\\\data\\\\deep-learning-images\\\\tulips\\\\3501996215_1c6d1a3386_n.jpg'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select(\"uri\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = load_image_from_uri(train_df.select(\"uri\").first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 299, 299, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n",
    "# This ensure that each of the paritions has a small size.\n",
    "# train_df = train_df.repartition(100)\n",
    "# test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorT = KerasImageFileEstimator( inputCol=\"uri\",\n",
    "                                     outputCol=\"prediction\",\n",
    "                                     labelCol=\"one_hot_label\",\n",
    "                                     imageLoader=load_image_from_uri,\n",
    "                                     kerasOptimizer='adam',\n",
    "                                     kerasLoss='categorical_crossentropy', # why categorical_crossentropy\n",
    "                                     modelFile=\"D:/tmp/model-full-tmp.h5\", # local file path for model\n",
    "                                     kerasFitParams = {\"batch_size\": 32, \"verbose\": 0}\n",
    "                                   ) \n",
    "\n",
    "# estimator = KerasImageFileEstimator(inputCol=\"imageUri\",\n",
    "#                                         outputCol=\"name_of_result_column\",\n",
    "#                                         labelCol=\"categoryVec\",\n",
    "#                                         imageLoader=load_image_and_process,\n",
    "#                                         kerasOptimizer=\"adam\",\n",
    "#                                         kerasLoss=\"categorical_crossentropy\",\n",
    "#                                         kerasFitParams={\"epochs\": 5, \"batch_size\": 64},\n",
    "#                                         modelFile=\"path_to_my_model.h5\")\n",
    "\n",
    "#     transformers = estimator.fit(image_dataset)\n",
    "# Init docstring:\n",
    "# __init__(self, inputCol=None, outputCol=None, \n",
    "#          outputMode=\"vector\", labelCol=None,\n",
    "#          modelFile=None, imageLoader=None, \n",
    "#          kerasOptimizer=None, kerasLoss=None,\n",
    "#          kerasFitParams=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   23|\n",
      "|  1.0|   19|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 495, localhost, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-5335f7e44250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfittedT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimatorT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mtuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitMultiple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36mfitMultiple\u001b[1;34m(self, dataset, paramMaps)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collectModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ThreadSafeIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_loadModelAsBytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, models)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0miterated\u001b[0m \u001b[0msafely\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m_collectModels\u001b[1;34m(self, kerasModelBytesRDD)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLlib\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m             \u001b[0mmodel_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytes_to_h5file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             yield i, self._copyValues(KerasImageFileTransformer(modelFile=model_filename),\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \"\"\"\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 495, localhost, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:428)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "fittedT = estimatorT.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasImageFileEstimator( inputCol=\"uri\",\n",
    "                                     outputCol=\"prediction\",\n",
    "                                     labelCol=\"one_hot_label\",\n",
    "                                     imageLoader=load_image_from_uri,\n",
    "                                     kerasOptimizer='adam',\n",
    "                                     kerasLoss='categorical_crossentropy', # why categorical_crossentropy\n",
    "                                     modelFile=\"D:/tmp/model-full-tmp1.h5\" # local file path for model\n",
    "                                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 529, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-33bc8521517c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mtuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitMultiple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36mfitMultiple\u001b[1;34m(self, dataset, paramMaps)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collectModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ThreadSafeIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_loadModelAsBytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, models)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0miterated\u001b[0m \u001b[0msafely\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m_collectModels\u001b[1;34m(self, kerasModelBytesRDD)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLlib\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m             \u001b[0mmodel_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytes_to_h5file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             yield i, self._copyValues(KerasImageFileTransformer(modelFile=model_filename),\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \"\"\"\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 529, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(32, 268203), b.shape=(268203, 2), m=32, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "fitted = estimator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "  ParamGridBuilder()\n",
    "  .addGrid(estimator.kerasFitParams, [{\"batch_size\": 16, \"verbose\": 0}])\n",
    "  .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = (\n",
    "#   ParamGridBuilder()\n",
    "#   .addGrid(estimator.kerasFitParams, [{\"batch_size\": 32, \"verbose\": 0},\n",
    "#                                       {\"batch_size\": 64, \"verbose\": 0}])\n",
    "#   .build()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\" )\n",
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=bc, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 512, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-5c81b141f4da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_parallelFitTasks\u001b[1;34m(est, train, eva, validation, epm)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m \u001b[0minto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mepm\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mmodelIter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitMultiple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36mfitMultiple\u001b[1;34m(self, dataset, paramMaps)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collectModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ThreadSafeIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_loadModelAsBytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, models)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miterator\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0miterated\u001b[0m \u001b[0msafely\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\u001b[0m in \u001b[0;36m_collectModels\u001b[1;34m(self, kerasModelBytesRDD)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLlib\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkerasModelBytesRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m             \u001b[0mmodel_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytes_to_h5file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             yield i, self._copyValues(KerasImageFileTransformer(modelFile=model_filename),\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \"\"\"\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-hadoop\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 512, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 316, in _local_fit\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 963, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n    validation_steps=validation_steps)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training.py\", line 1235, in _fit_loop\n    outs = f(ins_batch)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2478, in __call__\n    **self.session_kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\kranthikumar\\anaconda3\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 268, in <module>\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 229, in main\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 224, in process\n  File \"C:\\Spark\\spark-hadoop\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\estimators\\keras_image_file_estimator.py\", line 309, in _local_fit\n  File \"C:\\Users\\KranthiKumar\\AppData\\Local\\Temp\\spark-8f57aabc-7751-4da6-8eb7-848e14c9d104\\userFiles-4969ec9e-b28a-4d18-8564-f64be012b053\\databricks_spark-deep-learning-1.2.0-spark2.3-s_2.11.jar\\sparkdl\\utils\\keras_model.py\", line 65, in bytes_to_model\n    model = _load_keras_hdf5_model(temp_path)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 144, in deserialize_keras_object\n    list(custom_objects.items())))\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 1373, in from_config\n    model.add(layer)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\layers\\core.py\", line 855, in call\n    output = K.dot(inputs, self.kernel)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1075, in dot\n    out = tf.matmul(x, y)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2064, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2790, in _mat_mul\n    name=name)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(16, 268203), b.shape=(268203, 2), m=16, n=2, k=268203\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](flatten_1/Reshape, dense_1/kernel/read)]]\n\t [[Node: dense_1/BiasAdd/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_63_dense_1/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel = cv.fit(train_df)\n",
    "\n",
    "# #\n",
    "# File \"d:\\learn\\.virtualenvs\\pyspark-lab\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 137, in standardize_input_data\n",
    "#     str(data_shape))\n",
    "# ValueError: Error when checking target: expected predictions to have shape (1000,) but got array with shape (2,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-lab",
   "language": "python",
   "name": "pyspark-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
