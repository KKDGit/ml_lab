{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# (1) Importing dependency\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360, 224, 224, 3) (1360, 17)\n"
     ]
    }
   ],
   "source": [
    "# (2) Get Data\n",
    "x, y = oxflower17.load_data(one_hot=True)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(x[0,:,:,:])\n",
    "# print(y[0,:])\n",
    "# print(x[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (3) Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\n",
    " strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                17017     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 28,096,769\n",
      "Trainable params: 28,075,633\n",
      "Non-trainable params: 21,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Started at: 2018-08-13 08:37:13.921107\n",
      "Train on 1088 samples, validate on 272 samples\n",
      "Epoch 1/200\n",
      "1088/1088 [==============================] - 56s 52ms/step - loss: 2.8954 - acc: 0.2188 - val_loss: 13.6292 - val_acc: 0.1029\n",
      "Epoch 2/200\n",
      "1088/1088 [==============================] - 49s 45ms/step - loss: 2.2112 - acc: 0.3658 - val_loss: 7.3647 - val_acc: 0.2610\n",
      "Epoch 3/200\n",
      "1088/1088 [==============================] - 55s 50ms/step - loss: 1.6667 - acc: 0.4660 - val_loss: 5.2797 - val_acc: 0.3272\n",
      "Epoch 4/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 1.7030 - acc: 0.4706 - val_loss: 4.5258 - val_acc: 0.2978\n",
      "Epoch 5/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 1.4274 - acc: 0.5239 - val_loss: 2.7697 - val_acc: 0.4596\n",
      "Epoch 6/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 1.3199 - acc: 0.5790 - val_loss: 3.7945 - val_acc: 0.3676\n",
      "Epoch 7/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 1.3077 - acc: 0.5689 - val_loss: 2.3671 - val_acc: 0.4485\n",
      "Epoch 8/200\n",
      "1088/1088 [==============================] - 52s 48ms/step - loss: 1.2055 - acc: 0.6029 - val_loss: 2.7880 - val_acc: 0.4044\n",
      "Epoch 9/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 0.9928 - acc: 0.6728 - val_loss: 2.2183 - val_acc: 0.4559\n",
      "Epoch 10/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 0.9111 - acc: 0.6893 - val_loss: 2.4176 - val_acc: 0.4265\n",
      "Epoch 11/200\n",
      "1088/1088 [==============================] - 49s 45ms/step - loss: 0.8152 - acc: 0.7325 - val_loss: 2.2187 - val_acc: 0.4926\n",
      "Epoch 12/200\n",
      "1088/1088 [==============================] - 49s 45ms/step - loss: 0.7651 - acc: 0.7445 - val_loss: 2.4466 - val_acc: 0.4706\n",
      "Epoch 13/200\n",
      "1088/1088 [==============================] - 55s 50ms/step - loss: 0.7887 - acc: 0.7298 - val_loss: 2.3780 - val_acc: 0.4743\n",
      "Epoch 14/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.7756 - acc: 0.7408 - val_loss: 2.2096 - val_acc: 0.5000\n",
      "Epoch 15/200\n",
      "1088/1088 [==============================] - 53s 49ms/step - loss: 0.6011 - acc: 0.7969 - val_loss: 3.0421 - val_acc: 0.4338\n",
      "Epoch 16/200\n",
      "1088/1088 [==============================] - 48s 44ms/step - loss: 0.5630 - acc: 0.8143 - val_loss: 5.0045 - val_acc: 0.3419\n",
      "Epoch 17/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.5551 - acc: 0.8097 - val_loss: 2.8743 - val_acc: 0.5184\n",
      "Epoch 18/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.5798 - acc: 0.8042 - val_loss: 2.7274 - val_acc: 0.4890\n",
      "Epoch 19/200\n",
      "1088/1088 [==============================] - 48s 45ms/step - loss: 0.4450 - acc: 0.8585 - val_loss: 3.7501 - val_acc: 0.4449\n",
      "Epoch 20/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 0.3913 - acc: 0.8658 - val_loss: 2.6510 - val_acc: 0.4559\n",
      "Epoch 21/200\n",
      "1088/1088 [==============================] - 52s 48ms/step - loss: 0.3043 - acc: 0.8961 - val_loss: 3.1021 - val_acc: 0.4706\n",
      "Epoch 22/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.2301 - acc: 0.9246 - val_loss: 2.8340 - val_acc: 0.5074\n",
      "Epoch 23/200\n",
      "1088/1088 [==============================] - 50s 46ms/step - loss: 0.2506 - acc: 0.9237 - val_loss: 2.8607 - val_acc: 0.4853\n",
      "Epoch 24/200\n",
      "1088/1088 [==============================] - 56s 51ms/step - loss: 0.2400 - acc: 0.9154 - val_loss: 2.9363 - val_acc: 0.4559\n",
      "Epoch 25/200\n",
      "1088/1088 [==============================] - 52s 47ms/step - loss: 0.2673 - acc: 0.9145 - val_loss: 3.5454 - val_acc: 0.4559\n",
      "Epoch 26/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.2791 - acc: 0.9026 - val_loss: 2.7090 - val_acc: 0.5184\n",
      "Epoch 27/200\n",
      "1088/1088 [==============================] - 71s 65ms/step - loss: 0.2815 - acc: 0.9118 - val_loss: 3.1673 - val_acc: 0.4779\n",
      "Epoch 28/200\n",
      "1088/1088 [==============================] - 49s 45ms/step - loss: 0.2382 - acc: 0.9145 - val_loss: 3.5112 - val_acc: 0.4890\n",
      "Epoch 29/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.2183 - acc: 0.9292 - val_loss: 3.4646 - val_acc: 0.4890\n",
      "Epoch 30/200\n",
      "1088/1088 [==============================] - 48s 44ms/step - loss: 0.2251 - acc: 0.9384 - val_loss: 2.6905 - val_acc: 0.5404\n",
      "Epoch 31/200\n",
      "1088/1088 [==============================] - 49s 45ms/step - loss: 0.2018 - acc: 0.9384 - val_loss: 3.6274 - val_acc: 0.5074\n",
      "Epoch 32/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.1798 - acc: 0.9476 - val_loss: 3.0029 - val_acc: 0.5331\n",
      "Epoch 33/200\n",
      "1088/1088 [==============================] - 52s 48ms/step - loss: 0.1322 - acc: 0.9550 - val_loss: 3.2830 - val_acc: 0.4706\n",
      "Epoch 34/200\n",
      "1088/1088 [==============================] - 51s 47ms/step - loss: 0.1455 - acc: 0.9540 - val_loss: 2.7120 - val_acc: 0.5441\n",
      "Epoch 35/200\n",
      "1088/1088 [==============================] - 52s 48ms/step - loss: 0.1326 - acc: 0.9467 - val_loss: 3.0949 - val_acc: 0.5294\n",
      "Epoch 36/200\n",
      "1088/1088 [==============================] - 54s 50ms/step - loss: 0.1077 - acc: 0.9623 - val_loss: 3.6118 - val_acc: 0.5404\n",
      "Epoch 37/200\n",
      "1088/1088 [==============================] - 65s 60ms/step - loss: 0.1139 - acc: 0.9651 - val_loss: 2.8080 - val_acc: 0.5441\n",
      "Epoch 38/200\n",
      "1088/1088 [==============================] - 95s 87ms/step - loss: 0.1137 - acc: 0.9660 - val_loss: 3.5730 - val_acc: 0.5074\n",
      "Epoch 39/200\n",
      " 256/1088 [======>.......................] - ETA: 1:03 - loss: 0.0771 - acc: 0.9688"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "print('Execution Started at: ' + str(t1))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x, y, batch_size=64, epochs=200, verbose=1, validation_split=0.2,shuffle=True)\n",
    "\n",
    "t2 = datetime.now()\n",
    "print('Execution Ended at: ' + str(t2))\n",
    "print('Time Taken: ' + str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-lab",
   "language": "python",
   "name": "tf-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
